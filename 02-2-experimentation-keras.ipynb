{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acceptable-quality",
   "metadata": {},
   "source": [
    "# 02.1 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/ai-platform-unified/docs/training/custom-training) to train a keras classifier to predict whether a given trip will result in a tip > 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-alabama",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moving-integrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "solid-guess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:apache_beam.typehints.native_type_compatibility:Using Any for unsupported type: typing.Sequence[~T]\n",
      "TensorFlow: 2.3.0\n",
      "TensorFlow Transform: 0.26.0\n",
      "Apache Beam: 2.28.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "import apache_beam as beam\n",
    "\n",
    "from model_src import data, features, preprocessing, model, defaults, trainer, exporter\n",
    "from dataflow_src import data_prep\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")\n",
    "print(f\"Apache Beam: {beam.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expanded-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us'\n",
    "\n",
    "DATASET_DISPLAYNAME = 'chicago_taxi_tips'\n",
    "CUSTOM_MODEL_DISPLAYNAME = f'{DATASET_DISPLAYNAME}_classifier_custom'\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "\n",
    "LOCAL_WORKSPACE = '_workspace'\n",
    "RAW_SCHEMA_DIR = 'model_src/raw_schema/schema.pbtxt'\n",
    "TRAINING_DIR = os.path.join(LOCAL_WORKSPACE, 'training_output')\n",
    "PREPROCESSING_DIR = os.path.join(LOCAL_WORKSPACE, 'preprocessing_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_WORKSPACE = True\n",
    "if tf.io.gfile.exists(LOCAL_WORKSPACE) and REMOVE_WORKSPACE:\n",
    "    print(\"Removing previous local workspace...\")\n",
    "    tf.io.gfile.rmtree(LOCAL_WORKSPACE)\n",
    "\n",
    "print(\"Creating new local workspace...\")\n",
    "tf.io.gfile.mkdir(LOCAL_WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-story",
   "metadata": {},
   "source": [
    "## Get Source Query from Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_query(dataset_display_name, data_split, limit):\n",
    "    \n",
    "    dataset_client = aip.DatasetServiceClient(client_options=client_options)\n",
    "    for dataset in dataset_client.list_datasets(parent=PARENT):\n",
    "        if dataset.display_name == dataset_display_name:\n",
    "            dataset_uri = dataset.name\n",
    "            break\n",
    "\n",
    "    dataset = dataset_client.get_dataset(name=dataset_uri)\n",
    "    bq_source_uri = dataset.metadata['inputConfig']['bigquerySource']['uri']\n",
    "    _, bq_dataset_name, bq_table_name = bq_source_uri.replace(\"g://\", \"\").split('.')\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT \n",
    "            CAST(trip_start_timestamp AS STRING) trip_start_timestamp,\n",
    "            IF(trip_month IS NULL, -1, trip_month) trip_month,\t\n",
    "            IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
    "            IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
    "            IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\t\n",
    "            IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
    "            IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
    "            IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
    "            IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
    "            IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
    "            IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
    "            IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross,\n",
    "            tip_bin\n",
    "        FROM {bq_dataset_name}.{bq_table_name} \n",
    "        WHERE data_split = '{data_split}' LIMIT {limit}\n",
    "    '''\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-pacific",
   "metadata": {},
   "source": [
    "## Test Data Preprocessing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(PREPROCESSING_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(PREPROCESSING_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(PREPROCESSING_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(PREPROCESSING_DIR):\n",
    "    print(\"Removing previous preprocessing outputs...\")\n",
    "    tf.io.gfile.rmtree(PREPROCESSING_DIR)\n",
    "print(\"Creating preprocessing outputs directory...\")\n",
    "tf.io.gfile.mkdir(PREPROCESSING_DIR)\n",
    "\n",
    "DATA_SPLIT = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "raw_data_query = get_source_query(\n",
    "    DATASET_DISPLAYNAME, DATA_SPLIT, LIMIT)\n",
    "\n",
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_schema_location': RAW_SCHEMA_DIR,\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artefact_dir': TRANSFORM_ARTEFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(LOCAL_WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "    'project': PROJECT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "data_prep.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {PREPROCESSING_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-monaco",
   "metadata": {},
   "source": [
    "## Test the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(TRAINING_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(TRAINING_DIR, 'export')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-tournament",
   "metadata": {},
   "source": [
    "### Read transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-chambers",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'eval/data-*.gz')\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} ({input_features[key].dtype}): {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-climate",
   "metadata": {},
   "source": [
    "### Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers = model.create_model_inputs()\n",
    "input_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.create_binary_classifier(tft_output, hyperparams)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    #show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 3\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    raw_schema_dir=RAW_SCHEMA_DIR,\n",
    "    tft_output_dir=TRANSFORM_ARTEFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(EXPORT_DIR):\n",
    "    tf.io.gfile.rmtree(EXPORT_DIR)\n",
    "    \n",
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_dir=RAW_SCHEMA_DIR,\n",
    "    tft_output_dir=TRANSFORM_ARTEFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_DIR)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": 12,\n",
    "    \"trip_hour\": 6,\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day_of_week\": 4,\n",
    "    \"trip_seconds\": 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-grade",
   "metadata": {},
   "source": [
    "## Train the Model on AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "violent-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_WORKSPACE = f\"gs://{BUCKET}/ucaip_demo/chicago_taxi\"\n",
    "PREPROCESSING_DIR = os.path.join(GCS_WORKSPACE, 'preprocessing_output')\n",
    "TRAINING_DIR = os.path.join(GCS_WORKSPACE, 'training_output')\n",
    "\n",
    "EXPORTED_DATA_PREFIX = os.path.join(PREPROCESSING_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(PREPROCESSING_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(PREPROCESSING_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-montana",
   "metadata": {},
   "source": [
    "### Preprocess the data using Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(PREPROCESSING_DIR):\n",
    "    print(\"Removing previous preprocessing outputs...\")\n",
    "    tf.io.gfile.rmtree(PREPROCESSING_DIR)\n",
    "print(\"Creating preprocessing outputs directory...\")\n",
    "tf.io.gfile.mkdir(PREPROCESSING_DIR)\n",
    "\n",
    "DATA_SPLIT = 'UNASSIGNED'\n",
    "LIMIT = 1000000\n",
    "\n",
    "args = {\n",
    "    #'runner': 'DataflowRunner',\n",
    "    'raw_schema_location': RAW_SCHEMA_DIR,\n",
    "    'raw_data_query': get_source_query(DATASET_DISPLAYNAME, DATA_SPLIT, LIMIT),\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artefact_dir': TRANSFORM_ARTEFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(GCS_WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(GCS_WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-schedule",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "data_prep.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {PREPROCESSING_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-container",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m model_src.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTEFACTS_DIR} \\\n",
    "    --num-epochs=1 \\\n",
    "    --hidden-units=32,32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(GCS_WORKSPACE, 'trainer_packages')\n",
    "\n",
    "!rm -r model_src/__pycache__/\n",
    "!rm -r model_src/.ipynb_checkpoints/\n",
    "!rm -r model_src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f custom_job.tar custom_job.tar.gz\n",
    "!mkdir custom_job\n",
    "!cp setup.py custom_job/\n",
    "!cp -r model_src custom_job/\n",
    "!tar cvf custom_job.tar custom_job\n",
    "!gzip custom_job.tar\n",
    "!gsutil cp custom_job.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r custom_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-watts",
   "metadata": {},
   "source": [
    "### Submit AI Platform custom training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RUNTIME='tf-cpu.2-3'\n",
    "TRAIN_IMAGE = f\"gcr.io/cloud-aiplatform/training/{TRAIN_RUNTIME}:latest\"\n",
    "\n",
    "def submit_custom_job(\n",
    "    job_client, \n",
    "    model_display_name,\n",
    "    trainer_package_uri,\n",
    "    training_dir,\n",
    "    trainer_args,\n",
    "):\n",
    "    \n",
    "    job_name = f\"train_{model_display_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    \n",
    "    worker_pool_spec = [\n",
    "        {\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": 'n1-standard-4',\n",
    "                \"accelerator_count\": 0\n",
    "            },\n",
    "            \"python_package_spec\": {\n",
    "                \"executor_image_uri\": TRAIN_IMAGE,\n",
    "                \"package_uris\": [trainer_package_uri],\n",
    "                \"python_module\": \"model_src.task\",\n",
    "                \"args\": trainer_args,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    custom_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"job_spec\": {\n",
    "            \"worker_pool_specs\": worker_pool_spec,\n",
    "            \"base_output_directory\": {\n",
    "                \"output_uri_prefix\": training_dir\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    job_client = aip.JobServiceClient(\n",
    "        client_options=client_options\n",
    "    )\n",
    "    \n",
    "    response = job_client.create_custom_job(\n",
    "        parent=PARENT, custom_job=custom_job)\n",
    "    \n",
    "    print(\"name:\", response.name)\n",
    "    print(\"display_name:\", response.display_name)\n",
    "    print(\"state:\", response.state)\n",
    "    print(\"create_time:\", response.create_time)\n",
    "    print(\"update_time:\", response.update_time)\n",
    "    return response.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_client = aip.JobServiceClient(\n",
    "    client_options=client_options)\n",
    "\n",
    "if tf.io.gfile.exists(TRAINING_DIR):\n",
    "    print(\"Removing previous training outputs...\")\n",
    "    tf.io.gfile.rmtree(TRAINING_DIR)\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX + \"/train/*\"}',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX + \"/eval/*\"}',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTEFACTS_DIR}',\n",
    "    f'--num-epochs={10}',\n",
    "    f'--learning-rate={0.001}',\n",
    "    f'--hidden-units=64,32'\n",
    "]\n",
    "\n",
    "job_id = submit_custom_job(\n",
    "    job_client=job_client, \n",
    "    model_display_name=CUSTOM_MODEL_DISPLAYNAME,\n",
    "    trainer_package_uri=os.path.join(TRAINER_PACKAGE_DIR, 'custom_job.tar.gz'),\n",
    "    training_dir=TRAINING_DIR,\n",
    "    trainer_args=trainer_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = job_client.get_custom_job(name=job_id)\n",
    "    if response.state == aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Training job completed. - Training Time:\", response.update_time - response.create_time)\n",
    "        break\n",
    "        print(\"Training job has not completed:\", response.state)\n",
    "    elif response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "        print(\"Training job failed!\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Training job is running.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-trinity",
   "metadata": {},
   "source": [
    "## Upload exported model to AI Platform Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amazing-marker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://ksalama-cloudml-us/ucaip_demo/chicago_taxi/training_output/model/\n",
      "gs://ksalama-cloudml-us/ucaip_demo/chicago_taxi/training_output/model/saved_model.pb\n",
      "gs://ksalama-cloudml-us/ucaip_demo/chicago_taxi/training_output/model/assets/\n",
      "gs://ksalama-cloudml-us/ucaip_demo/chicago_taxi/training_output/model/variables/\n"
     ]
    }
   ],
   "source": [
    "exported_model_dir = os.path.join(TRAINING_DIR, 'model')\n",
    "\n",
    "!gsutil ls {exported_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-guess",
   "metadata": {},
   "source": [
    "### Upload predict schemata yaml files to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-recipe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_SCHEMATA_DIR = os.path.join(GCS_WORKSPACE, 'predict_schemata')\n",
    "!gsutil cp predict_schemata/* {PREDICT_SCHEMATA_DIR}\n",
    "!gsutil ls {PREDICT_SCHEMATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "occasional-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-3'\n",
    "SERVING_IMAGE = f\"gcr.io/cloud-aiplatform/prediction/{SERVING_RUNTIME}:latest\"\n",
    "\n",
    "model = {\n",
    "    \"display_name\": CUSTOM_MODEL_DISPLAYNAME,\n",
    "    \"artifact_uri\": exported_model_dir,\n",
    "#     \"predict_schemata\": {\n",
    "#         \"instance_schema_uri\": os.path.join(PREDICT_SCHEMATA_DIR, 'instance_schema.yaml'),\n",
    "#         \"prediction_schema_uri\": os.path.join(PREDICT_SCHEMATA_DIR, 'prediction_schema.yaml'),\n",
    "    \n",
    "#     },\n",
    "    \"container_spec\": {\n",
    "        \"image_uri\": SERVING_IMAGE,\n",
    "        \"command\": [],\n",
    "        \"args\": [],\n",
    "        \"env\": [{\"name\": \"env_name\", \"value\": \"env_value\"}],\n",
    "        \"ports\": [{\"container_port\": 8080}],\n",
    "        \"predict_route\": \"\",\n",
    "        \"health_route\": \"\",\n",
    "    },\n",
    "#    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/model/metadata/automl_tabular_1.0.0.yaml\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-disclaimer",
   "metadata": {},
   "source": [
    "### Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "understood-qatar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: \"projects/900786220115/locations/us-central1/models/2233169688664211456\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_client = aip.ModelServiceClient(client_options=client_options)\n",
    "\n",
    "response = model_client.upload_model(\n",
    "    model=model,\n",
    "    parent=PARENT\n",
    ")\n",
    "\n",
    "# response.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distinct-latex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/900786220115/locations/us-central1/models/2233169688664211456'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_result = response.result()\n",
    "response_result.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = model_client.list_models(parent=PARENT)\n",
    "\n",
    "for entry in model_list:\n",
    "    if entry.display_name == CUSTOM_MODEL_DISPLAYNAME:\n",
    "        model_uri = entry.name\n",
    "        break\n",
    "\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-chemistry",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
