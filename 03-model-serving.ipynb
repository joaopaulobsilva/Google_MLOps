{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "champion-radiation",
   "metadata": {},
   "source": [
    "# 03 - Model Serving\n",
    "\n",
    "The purpose of the notebook is to show how to serve both AutoML Tables and Custom models for online and batch prediction.\n",
    "The notebook covers the following tasks:\n",
    "1. Creating an AI Platform Endpoint\n",
    "2. Deploy the AutoML Tables and the custom modesl to the endpoint.\n",
    "4. Test the endpoints for online prediction.\n",
    "5. Getting online explaination from the AutoML Tables mode.\n",
    "5. Use the uploaded custom model for batch prediciton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-conducting",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'  # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us' # Change to your bucket.\n",
    "\n",
    "DATASET_DISPLAY_NAME = 'chicago_taxi_tips'\n",
    "MODEL_ENDPOINT_DISPLAY_NAME = 'chicago_taxi_tips_classifier'\n",
    "AUTOML_MODEL_DISPLAY_NAME = 'chicago_taxi_tips_classifier_automl'\n",
    "CUSTOM_MODEL_DISPLAY_NAME = 'chicago_taxi_tips_classifier_custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ucaip_utils import AIPUtils\n",
    "aip_utils = AIPUtils(PROJECT, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-buyer",
   "metadata": {},
   "source": [
    "## 1. Create AI Platform Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = aip_utils.create_endpoint(MODEL_ENDPOINT_DISPLAY_NAME)\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-freeze",
   "metadata": {},
   "source": [
    "## 2. Deploy AI Platform Model to Endpoint\n",
    "\n",
    "We assume that both the AutoML Tables model and the custom model have the same serving signature to be deployed under the same Endpoint. In this case, we can split the traffic between them (for example, to perform A/B testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedicated_serving_resources_spec = { \n",
    "    'machine_spec': {\n",
    "        'machine_type': 'n1-standard-2',\n",
    "        #'accelerator_count': 1,\n",
    "        #'accelerator_type': 'NVIDIA_TESLA_T4'\n",
    "    },\n",
    "    'min_replica_count': 1,\n",
    "    'max_replica_count': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-hindu",
   "metadata": {},
   "source": [
    "### Deploy AutoML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = aip_utils.deploy_model(\n",
    "        model_display_name=AUTOML_MODEL_DISPLAY_NAME,\n",
    "        endpoint_display_name=MODEL_ENDPOINT_DISPLAY_NAME,\n",
    "        dedicated_serving_resources_spec=dedicated_serving_resources_spec,\n",
    "    )\n",
    "\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-wellington",
   "metadata": {},
   "source": [
    "### Deploy Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = aip_utils.deploy_model(\n",
    "    model_display_name=CUSTOM_MODEL_DISPLAY_NAME,\n",
    "    endpoint_display_name=MODEL_ENDPOINT_DISPLAY_NAME,\n",
    "    dedicated_serving_resources_spec=dedicated_serving_resources_spec,\n",
    "    traffic_split={\"0\": 50, \"1\": 50}\n",
    ")\n",
    "\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-boards",
   "metadata": {},
   "source": [
    "** How to update traffic split progammatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-interaction",
   "metadata": {},
   "source": [
    "## 3. Making Online Predicitons\n",
    "\n",
    "** Currently the AutoML Tables and the Custom model don't have the same serving signature, so they expect two differnt types of the input instances. However, the endpoint would only accept the instance of the first deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = aip_utils.get_endpoint_by_display_name(\n",
    "    MODEL_ENDPOINT_DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_automl = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": \"12\",\n",
    "    \"trip_hour\": \"16\",\n",
    "    \"trip_month\": \"2\",\n",
    "    \"trip_day_of_week\": \"4\",\n",
    "    \"trip_seconds\": \"555\",\n",
    "}\n",
    "\n",
    "instance_custom = {\n",
    "    \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "    \"euclidean\": [2064.2696],\n",
    "    \"loc_cross\": [\"\"],\n",
    "    \"payment_type\": [\"Credit Card\"],\n",
    "    \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "    \"trip_miles\": [1.37],\n",
    "    \"trip_day\": [12],\n",
    "    \"trip_hour\": [16],\n",
    "    \"trip_month\": [2],\n",
    "    \"trip_day_of_week\": [4],\n",
    "    \"trip_seconds\": [555],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        response = aip_utils.predict_tabular_classifier(\n",
    "            endpoint.name, \n",
    "            instance_automl\n",
    "        )\n",
    "        print(f\"AutoML model (id: {response.deployed_model_id}) responded:\")\n",
    "        for prediction in response.predictions:\n",
    "            print(dict(prediction))\n",
    "\n",
    "    except:\n",
    "        response = aip_utils.predict_tabular_classifier(\n",
    "            endpoint.name,  \n",
    "            instance_custom\n",
    "        )\n",
    "        print(f\"Custom model (id: {response.deployed_model_id}) responded:\")\n",
    "        for prediction in response.predictions:\n",
    "            print(dict(prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-madagascar",
   "metadata": {},
   "source": [
    "### 4. Getting Online Explaination (AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = aip_utils.explain_tabular_classifier(\n",
    "        endpoint.name, \n",
    "        instance_automl,\n",
    "    )\n",
    "    print(\"AutoML model responded:\")\n",
    "    print(response.explanations)\n",
    "except:\n",
    "     print(\"Custom model responded: No support for explaination.\")\n",
    "    \n",
    "#     response = explain_tabular_classifier(\n",
    "#         client_options, \n",
    "#         model_endpoint.name, \n",
    "#         instance_custom,\n",
    "#     )\n",
    "#     print(\"Custom model responded:\")\n",
    "#     for explaination in response.explainations:\n",
    "#         print(dict(explaination))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-internet",
   "metadata": {},
   "source": [
    "## 5. Batch Prediction (Custom Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = f\"gs://{BUCKET}/ucaip_demo/chicago_taxi\"\n",
    "SERVING_DATA_DIR = os.path.join(WORKSPACE, 'serving_data')\n",
    "SERVING_INPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'input_data')\n",
    "SERVING_OUTPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'output_predictions')\n",
    "\n",
    "RAW_SCHEMA_DIR = 'model_src/raw_schema/schema.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(SERVING_DATA_DIR):\n",
    "    print(\"Removing previous serving data...\")\n",
    "    tf.io.gfile.rmtree(SERVING_DATA_DIR)\n",
    "print(\"Creating preprocessing serving data directory...\")\n",
    "tf.io.gfile.mkdir(SERVING_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-choir",
   "metadata": {},
   "source": [
    "### Extract serving data to Cloud Storage as TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import datasource_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'TEST'\n",
    "LIMIT = 10000\n",
    "\n",
    "raw_data_query = datasource_utils.get_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    data_split=DATA_SPLIT, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    #'runner': 'DataflowRunner',\n",
    "    'raw_schema_location': RAW_SCHEMA_DIR,\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': os.path.join(SERVING_INPUT_DATA_DIR, \"data-\"),\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflow_src import data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Data extraction started...\")\n",
    "data_prep.run_extract_pipeline(args)\n",
    "print(\"Data extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_INPUT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-enemy",
   "metadata": {},
   "source": [
    "### Prepare the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedicated_resources =  {\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": 'n1-standard-2',\n",
    "        #'accelerator_count': 1,\n",
    "        #'accelerator_type': 'NVIDIA_TESLA_T4'\n",
    "    },\n",
    "    \"starting_replica_count\": 1,\n",
    "    \"max_replica_count\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-belize",
   "metadata": {},
   "source": [
    "### Submit the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = aip_utils.submit_batch_prediction_job(\n",
    "        model_display_name=CUSTOM_MODEL_DISPLAY_NAME, \n",
    "        gcs_data_uri_pattern=SERVING_INPUT_DATA_DIR + '/*.jsonl', \n",
    "        gcs_output_uri=SERVING_OUTPUT_DATA_DIR,\n",
    "        dedicated_resources=dedicated_resources,\n",
    "        instances_format='jsonl',\n",
    "        predictions_format='jsonl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-spelling",
   "metadata": {},
   "source": [
    "### Monitor job state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = aip_utils.get_batch_prediction_job_by_uri(batch_prediction_job.name)\n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Batch prediction completed. - Training Time:\", response.update_time - response.create_time)\n",
    "        break\n",
    "    elif response.state.name == 'JOB_STATE_FAILED':\n",
    "        print(\"Batch prediction failed!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Batch prediction state is: {response.state.name}.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_OUTPUT_DATA_DIR}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
