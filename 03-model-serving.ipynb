{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parental-exclusive",
   "metadata": {},
   "source": [
    "# 03 - Model Serving\n",
    "\n",
    "The purpose of the notebook is to show how to serve both AutoML Tables and Custom models for online and batch prediction.\n",
    "The notebook covers the following tasks:\n",
    "1. Creating an AI Platform Endpoint\n",
    "2. Deploy the AutoML Tables and the custom modesl to the endpoint.\n",
    "4. Test the endpoints for online prediction.\n",
    "5. Getting online explaination from the AutoML Tables mode.\n",
    "5. Use the uploaded custom model for batch prediciton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-passion",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from google.cloud.aiplatform import gapic as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us'\n",
    "\n",
    "MODEL_ENDPOINT_DISPLAYNAME = 'chicago_taxi_tips_classifier'\n",
    "AUTOML_MODEL_DISPLAYNAME = 'chicago_taxi_tips_classifier_automl'\n",
    "CUSTOM_MODEL_DISPLAYNAME = 'chicago_taxi_tips_classifier_custom'\n",
    "\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-sunset",
   "metadata": {},
   "source": [
    "## 1. Create AI Platform Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_client = aip.EndpointServiceClient(client_options=client_options)\n",
    "model_client = aip.ModelServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoint_client.create_endpoint(\n",
    "    parent=PARENT,\n",
    "    endpoint=aip.Endpoint(display_name=MODEL_ENDPOINT_DISPLAYNAME)\n",
    ")\n",
    "\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in endpoint_client.list_endpoints(parent=PARENT):\n",
    "    if entry.display_name == MODEL_ENDPOINT_DISPLAYNAME:\n",
    "        model_endpoint = entry\n",
    "        break\n",
    "        \n",
    "model_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-orchestra",
   "metadata": {},
   "source": [
    "## 2. Deploy AI Platform Model to Endpoint\n",
    "\n",
    "We assume that both the AutoML Tables model and the custom model have the same serving signature to be deployed under the same Endpoint. In this case, we can split the traffic between them (for example, to perform A/B testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_to_endpoint(\n",
    "    model_display_name,\n",
    "    endpoint_display_name,\n",
    "    model_client,\n",
    "    endpoint_client,\n",
    "    parent,\n",
    "    traffic_split\n",
    "):\n",
    "    \n",
    "    for entry in model_client.list_models(parent=parent):\n",
    "        if entry.display_name == model_display_name:\n",
    "            aip_model = entry\n",
    "            \n",
    "    for entry in endpoint_client.list_endpoints(parent=parent):\n",
    "        if entry.display_name == endpoint_display_name:\n",
    "            model_endpoint = entry\n",
    "            break\n",
    "\n",
    "\n",
    "    serving_machine_spec = aip.MachineSpec(\n",
    "        machine_type='n1-standard-2',\n",
    "        #accelerator_count=1,\n",
    "        #accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4\n",
    "    )\n",
    "\n",
    "    dedicated_serving_resources = aip.DedicatedResources(\n",
    "        machine_spec=serving_machine_spec,\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=5\n",
    "    )\n",
    "\n",
    "    deployed_model = aip.DeployedModel(\n",
    "        model=aip_model.name,\n",
    "        disable_container_logging=False,\n",
    "        enable_access_logging=False,\n",
    "        #automatic_resources=None,\n",
    "        dedicated_resources=dedicated_serving_resources\n",
    "    )\n",
    "    \n",
    "    response = endpoint_client.deploy_model(\n",
    "        endpoint=model_endpoint.name,\n",
    "        deployed_model=deployed_model,\n",
    "        traffic_split=traffic_split\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-pleasure",
   "metadata": {},
   "source": [
    "### Deploy AutoML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy AutoML Model\n",
    "response = deploy_model_to_endpoint(\n",
    "    AUTOML_MODEL_DISPLAYNAME,\n",
    "    MODEL_ENDPOINT_DISPLAYNAME,\n",
    "    model_client,\n",
    "    endpoint_client,\n",
    "    PARENT,\n",
    "    {\"0\": 100}\n",
    ")\n",
    "\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-objective",
   "metadata": {},
   "source": [
    "### Deploy Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = deploy_model_to_endpoint(\n",
    "    CUSTOM_MODEL_DISPLAYNAME,\n",
    "    MODEL_ENDPOINT_DISPLAYNAME,\n",
    "    model_client,\n",
    "    endpoint_client,\n",
    "    PARENT,\n",
    "    {\"0\": 50, \"1\": 50}\n",
    ")\n",
    "\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accurate-software",
   "metadata": {},
   "source": [
    "** How to update traffic split progammatically?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-alexander",
   "metadata": {},
   "source": [
    "## 3. Making Online Predicitons\n",
    "\n",
    "** Currently the AutoML Tables and the Custom model don't have the same serving signature, so they expect two differnt types of the input instances. However, the endpoint would only accept the instance of the first deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_tabular_classifier(\n",
    "    client_options,\n",
    "    endpoint,\n",
    "    instance\n",
    "):\n",
    "    instances = [json_format.ParseDict(instance, Value())]\n",
    "    prediction_client = aip.PredictionServiceClient(client_options=client_options)\n",
    "    response = prediction_client.predict(\n",
    "        endpoint=endpoint, instances=instances#, parameters=parameters\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_automl = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": \"12\",\n",
    "    \"trip_hour\": \"16\",\n",
    "    \"trip_month\": \"2\",\n",
    "    \"trip_day_of_week\": \"4\",\n",
    "    \"trip_seconds\": \"555\",\n",
    "}\n",
    "\n",
    "# instance_custom = {\n",
    "#     \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "#     \"euclidean\": 2064.2696,\n",
    "#     \"loc_cross\": \"\",\n",
    "#     \"payment_type\": \"Credit Card\",\n",
    "#     \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "#     \"trip_miles\": 1.37,\n",
    "#     \"trip_day\": 12,\n",
    "#     \"trip_hour\": 16,\n",
    "#     \"trip_month\": 2,\n",
    "#     \"trip_day_of_week\": 4,\n",
    "#     \"trip_seconds\": 555,\n",
    "# }\n",
    "\n",
    "instance_custom = {\n",
    "    \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "    \"euclidean\": [2064.2696],\n",
    "    \"loc_cross\": [\"\"],\n",
    "    \"payment_type\": [\"Credit Card\"],\n",
    "    \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "    \"trip_miles\": [1.37],\n",
    "    \"trip_day\": [12],\n",
    "    \"trip_hour\": [16],\n",
    "    \"trip_month\": [2],\n",
    "    \"trip_day_of_week\": [4],\n",
    "    \"trip_seconds\": [555],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    try:\n",
    "        response = predict_tabular_classifier(\n",
    "            client_options, \n",
    "            model_endpoint.name, \n",
    "            instance_automl\n",
    "        )\n",
    "        print(f\"AutoML model (id: {response.deployed_model_id}) responded:\")\n",
    "        for prediction in response.predictions:\n",
    "            print(dict(prediction))\n",
    "\n",
    "    except:\n",
    "        response = predict_tabular_classifier(\n",
    "            client_options, \n",
    "            model_endpoint.name, \n",
    "            instance_custom\n",
    "        )\n",
    "        print(f\"Custom model (id: {response.deployed_model_id}) responded:\")\n",
    "        for prediction in response.predictions:\n",
    "            print(dict(prediction))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-elements",
   "metadata": {},
   "source": [
    "### 4. Getting Online Explaination (AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform_v1beta1 as aip_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_tabular_classifier(\n",
    "    client_options,\n",
    "    endpoint,\n",
    "    instance\n",
    "):\n",
    "    instances = [json_format.ParseDict(instance, Value())]\n",
    "    prediction_client = aip_beta.PredictionServiceClient(client_options=client_options)\n",
    "    response = prediction_client.explain(\n",
    "        endpoint=endpoint, \n",
    "        instances=instances\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = explain_tabular_classifier(\n",
    "        client_options, \n",
    "        model_endpoint.name, \n",
    "        instance_automl,\n",
    "    )\n",
    "    print(\"AutoML model responded:\")\n",
    "    for explaination in response.explainations:\n",
    "        print(dict(explaination))\n",
    "except:\n",
    "     print(\"Custom model responded: No support for explaination.\")\n",
    "    \n",
    "#     response = explain_tabular_classifier(\n",
    "#         client_options, \n",
    "#         model_endpoint.name, \n",
    "#         instance_custom,\n",
    "#     )\n",
    "#     print(\"Custom model responded:\")\n",
    "#     for explaination in response.explainations:\n",
    "#         print(dict(explaination))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-handbook",
   "metadata": {},
   "source": [
    "## 5. Batch Prediction (Custom Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_WORKSPACE = f\"gs://{BUCKET}/ucaip_demo/chicago_taxi\"\n",
    "SERVING_DATA_DIR = os.path.join(GCS_WORKSPACE, 'serving_data')\n",
    "SERVING_INPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'input_data')\n",
    "SERVING_OUTPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'output_predictions')\n",
    "\n",
    "RAW_SCHEMA_DIR = 'model_src/raw_schema/schema.pbtxt'\n",
    "DATASET_DISPLAYNAME = 'chicago_taxi_tips'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(SERVING_DATA_DIR):\n",
    "    print(\"Removing previous serving data...\")\n",
    "    tf.io.gfile.rmtree(SERVING_DATA_DIR)\n",
    "print(\"Creating preprocessing serving data directory...\")\n",
    "tf.io.gfile.mkdir(SERVING_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-running",
   "metadata": {},
   "source": [
    "### Extract serving data to Cloud Storage as TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'TEST'\n",
    "LIMIT = 10000\n",
    "\n",
    "def get_source_query(dataset_display_name, data_split, limit):\n",
    "    \n",
    "    dataset_client = aip.DatasetServiceClient(client_options=client_options)\n",
    "    for dataset in dataset_client.list_datasets(parent=PARENT):\n",
    "        if dataset.display_name == dataset_display_name:\n",
    "            dataset_uri = dataset.name\n",
    "            break\n",
    "\n",
    "    dataset = dataset_client.get_dataset(name=dataset_uri)\n",
    "    bq_source_uri = dataset.metadata['inputConfig']['bigquerySource']['uri']\n",
    "    _, bq_dataset_name, bq_table_name = bq_source_uri.replace(\"g://\", \"\").split('.')\n",
    "    \n",
    "    query = f'''\n",
    "        SELECT \n",
    "            IF(trip_month IS NULL, -1, trip_month) trip_month,\t\n",
    "            IF(trip_day IS NULL, -1, trip_day) trip_day,\n",
    "            IF(trip_day_of_week IS NULL, -1, trip_day_of_week) trip_day_of_week,\n",
    "            IF(trip_hour IS NULL, -1, trip_hour) trip_hour,\t\n",
    "            IF(trip_seconds IS NULL, -1, trip_seconds) trip_seconds,\n",
    "            IF(trip_miles IS NULL, -1, trip_miles) trip_miles,\n",
    "            IF(payment_type IS NULL, 'NA', payment_type) payment_type,\n",
    "            IF(pickup_grid IS NULL, 'NA', pickup_grid) pickup_grid,\n",
    "            IF(dropoff_grid IS NULL, 'NA', dropoff_grid) dropoff_grid,\n",
    "            IF(euclidean IS NULL, -1, euclidean) euclidean,\n",
    "            IF(loc_cross IS NULL, 'NA', loc_cross) loc_cross\n",
    "        FROM {bq_dataset_name}.{bq_table_name} \n",
    "        WHERE data_split = '{data_split}' LIMIT {limit}\n",
    "    '''\n",
    "    return query\n",
    "\n",
    "\n",
    "args = {\n",
    "    #'runner': 'DataflowRunner',\n",
    "    'raw_schema_location': RAW_SCHEMA_DIR,\n",
    "    'raw_data_query': get_source_query(DATASET_DISPLAYNAME, DATA_SPLIT, LIMIT),\n",
    "    'exported_data_prefix': os.path.join(SERVING_INPUT_DATA_DIR, \"data-\"),\n",
    "    'temporary_dir': os.path.join(GCS_WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(GCS_WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflow_src import data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-jewel",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Data extraction started...\")\n",
    "data_prep.run_extract_pipeline(args)\n",
    "print(\"Data extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_INPUT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-expansion",
   "metadata": {},
   "source": [
    "### Prepare the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_prediction_job(\n",
    "    job_client,\n",
    "    model_display_name, \n",
    "    gcs_data_uri_pattern, \n",
    "    gcs_output_uri,\n",
    "    parent\n",
    "):\n",
    "    \n",
    "    serving_data_uris = tf.io.gfile.glob(gcs_data_uri_pattern)\n",
    "    \n",
    "    for entry in model_client.list_models(parent=parent):\n",
    "        if entry.display_name == model_display_name:\n",
    "            aip_model = entry\n",
    "    \n",
    "    job_name = f\"batch_predict_{model_display_name}_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "    machine_spec = {\n",
    "        \"machine_type\": 'n1-standard-2',\n",
    "        #accelerator_count=1,\n",
    "        #accelerator_type=aip.AcceleratorType.NVIDIA_TESLA_T4\n",
    "    }\n",
    "\n",
    "    batch_prediction_job = {\n",
    "        \"display_name\": job_name,\n",
    "        \"model\": aip_model.name,\n",
    "        #\"model_parameters\": json_format.ParseDict(model_parameters, Value()),\n",
    "        \"input_config\": {\n",
    "            \"instances_format\": \"jsonl\",\n",
    "            \"gcs_source\": {\"uris\": serving_data_uris},\n",
    "        },\n",
    "        \"output_config\": {\n",
    "            \"predictions_format\": \"jsonl\",\n",
    "            \"gcs_destination\": {\"output_uri_prefix\": gcs_output_uri},\n",
    "        },\n",
    "        \"dedicated_resources\": {\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"starting_replica_count\": 1,\n",
    "            \"max_replica_count\": 10,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    response = job_client.create_batch_prediction_job(\n",
    "        parent=PARENT, batch_prediction_job=batch_prediction_job\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-suggestion",
   "metadata": {},
   "source": [
    "### Submit the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_client = aip.JobServiceClient(client_options=client_options)\n",
    "\n",
    "batch_prediction_job = create_batch_prediction_job(\n",
    "    job_client,\n",
    "    CUSTOM_MODEL_DISPLAYNAME, \n",
    "    SERVING_INPUT_DATA_DIR + '/*.jsonl', \n",
    "    SERVING_OUTPUT_DATA_DIR,\n",
    "    PARENT\n",
    ")\n",
    "\n",
    "batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = job_client.get_batch_prediction_job(name=batch_prediction_job.name)\n",
    "    if response.state == aip.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Batch prediction job completed. - Training Time:\", response.update_time - response.create_time)\n",
    "        break\n",
    "        print(\"Training job has not completed:\", response.state)\n",
    "    elif response.state == aip.JobState.JOB_STATE_FAILED:\n",
    "        print(\"Batch prediction job failed!\")\n",
    "        break\n",
    "    else:\n",
    "        print(\"Batch prediction job is running.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_OUTPUT_DATA_DIR}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
