{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8277739",
   "metadata": {},
   "source": [
    "# 02 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training) to train a TF classifier to predict whether a given trip will result in a tip > 20%. The notebook covers the following tasks:\n",
    "1. Preprocess the data locally using `Apache Beam`.\n",
    "2. Train and test a TensorFlow custom model locally.\n",
    "3. Submit a `Dataflow` job to preprocess the data at scale.\n",
    "4. Submit a `Vertex` custom training job using a [pre-built container](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers).\n",
    "5. Upload the trained model to `Vertex Model` resource\n",
    "6. Extract and visualize experiment parameters from [Vertex Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction).\n",
    "\n",
    "We use [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) \n",
    "and [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) to  track, visualize, and compare ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c861b30",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063065de",
   "metadata": {},
   "source": [
    "Install the latest GA version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553afafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05232ef",
   "metadata": {},
   "source": [
    "Install deep learning dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U tfx==0.30.0 $USER_FLAG\n",
    "! pip3 install -r requirements.txt $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f520504",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the Vertex SDK and Google *cloud-storage*, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2edcd1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import aiplatform_v1beta1 as vertex_ai_beta\n",
    "\n",
    "from src.common import features\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.utils import datasource_utils\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb5b61",
   "metadata": {},
   "source": [
    "### Setup your Google Cloud project\n",
    "\n",
    "Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf21884",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)\n",
    "    \n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cfa32e",
   "metadata": {},
   "source": [
    "Enter your service account in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27b1fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844fb58",
   "metadata": {},
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex. Not all regions provide support for all Vertex services. For the latest support per region, see the [Vertex locations documentation](https://cloud.google.com/ai-platform-unified/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de46213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7e37b9",
   "metadata": {},
   "source": [
    "### Setup a bucket for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b305ec2",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de8d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fd18e",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255670bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eff1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DISPLAY_NAME = 'chicago_taxi_tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}_classifier'\n",
    "\n",
    "WORKSPACE = f'{BUCKET_NAME}/vertex_demo'\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT_ID}'\n",
    "EXPERIMENT_NAME = f'{DATASET_DISPLAY_NAME}_experiment'.replace('_', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a733fb2",
   "metadata": {},
   "source": [
    "## Create `Vertex TensorBoard` instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud alpha ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT_ID} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cdd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT=f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "tensorboard_client_beta = vertex_ai_beta.TensorboardServiceClient(\n",
    "            client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    "        )\n",
    "\n",
    "def list_tensorboards(filter):\n",
    "    key, value = filter.split(\"=\")\n",
    "    tensorboards = tensorboard_client_beta.list_tensorboards(parent=PARENT)\n",
    "    \n",
    "    ret = []\n",
    "    for tensorboard in tensorboards:\n",
    "        if key == \"display_name\":\n",
    "            if value == tensorboard.display_name:\n",
    "                ret.append(tensorboard)\n",
    "    return ret\n",
    "        \n",
    "tensorboards = list_tensorboards(filter=f\"display_name={TENSORBOARD_DISPLAY_NAME}\")\n",
    "tensorboard = tensorboards[0]\n",
    "tensorboard_resource_name = tensorboard.name\n",
    "\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f661e6",
   "metadata": {},
   "source": [
    "## Initialize Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print(\"Removing previous experiment artifacts...\")\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print(\"Creating new experiment artifacts directory...\")\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print(\"Workspace is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acd639",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0589f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "            project=PROJECT_ID,\n",
    "            staging_bucket=BUCKET_NAME,\n",
    "            experiment=EXPERIMENT_NAME,\n",
    "        )\n",
    "\n",
    "\n",
    "run_id = f\"run-local-{TIMESTAMP}\".replace('_', '-')\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169c3124",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data using `Apache Beam`\n",
    "\n",
    "The Apache Beam pipeline of data preprocessing is implemented in the [preprocessing](src/preprocessing) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fcb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c17f8",
   "metadata": {},
   "source": [
    "### Get Source Query from `Dataset` resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d271c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT_ID, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    ML_use=DATA_SPLIT, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab5b9f",
   "metadata": {},
   "source": [
    "### Test Data Preprocessing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226ce72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'{BUCKET_NAME}/bq_tmp',\n",
    "    'project': PROJECT_ID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e209c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b6ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb498c",
   "metadata": {},
   "source": [
    "## 2. Train a custom TF.Keras model locally\n",
    "\n",
    "The implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719fd31",
   "metadata": {},
   "source": [
    "### Read transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afe39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c0b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'eval/data-*.gz')\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662d042",
   "metadata": {},
   "source": [
    "### Create hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a4dfa",
   "metadata": {},
   "source": [
    "### Create and test model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15925fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.create_binary_classifier(tft_output, hyperparams)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbabdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3748cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e7d37",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f74250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 5\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fda765",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(\n",
    "    model=classifier,\n",
    "    data_dir=eval_data_file_pattern,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4d7df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_metrics(\n",
    "    {\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8890def",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa36fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3856a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a27f81f",
   "metadata": {},
   "source": [
    "### Test the exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173ac78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '/data-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8340498",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": 12,\n",
    "    \"trip_hour\": 6,\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day_of_week\": 4,\n",
    "    \"trip_seconds\": 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7a6d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49fc3c",
   "metadata": {},
   "source": [
    "## Start a new Vertex AI Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ed570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "            project=PROJECT_ID,\n",
    "            staging_bucket=BUCKET_NAME,\n",
    "            experiment=EXPERIMENT_NAME,\n",
    "        )\n",
    "\n",
    "\n",
    "run_id = f\"run-local-{TIMESTAMP}\".replace('_', '-')\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803e3ad",
   "metadata": {},
   "source": [
    "## 3. Submit a Data Processing Job to Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcbd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e3fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'UNASSIGNED'\n",
    "LIMIT = 1000000\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT_ID, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    ML_use=DATA_SPLIT, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'runner': 'DataflowRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cebce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fe460",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773de85e",
   "metadata": {},
   "source": [
    "## 4. Submit a Custom Training Job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b5c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361e657",
   "metadata": {},
   "source": [
    "### Test the training task locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTIFACTS_DIR} \\\n",
    "    --num-epochs=5 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e065e4b",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')\n",
    "TRAINER_PACKAGE_NAME = f'{MODEL_DISPLAY_NAME}_trainer'\n",
    "TRAINER_PACKAGE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc79b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r src/__pycache__/\n",
    "! rm -r src/.ipynb_checkpoints/\n",
    "! rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "! rm -f {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}.tar.gz\n",
    "\n",
    "! mkdir {TRAINER_PACKAGE_NAME}\n",
    "\n",
    "! cp setup.py {TRAINER_PACKAGE_NAME}/\n",
    "! cp -r src {TRAINER_PACKAGE_NAME}/\n",
    "! tar cvf {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}\n",
    "! gzip {TRAINER_PACKAGE_NAME}.tar\n",
    "! gsutil cp {TRAINER_PACKAGE_NAME}.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "! rm -r {TRAINER_PACKAGE_NAME}\n",
    "! rm -r {TRAINER_PACKAGE_NAME}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203715b",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c30cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RUNTIME = 'tf-cpu.2-4'\n",
    "TRAIN_IMAGE = f\"gcr.io/cloud-aiplatform/training/{TRAIN_RUNTIME}:latest\"\n",
    "print(TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_units = \"64,64\"\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX + \"/train/*\"}',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX + \"/eval/*\"}',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTIFACTS_DIR}',\n",
    "    f'--num-epochs={num_epochs}',\n",
    "    f'--learning-rate={learning_rate}',\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--region={REGION}',\n",
    "    f'--staging-bucket={BUCKET_NAME}',\n",
    "    f'--experiment-name={EXPERIMENT_NAME}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bed719",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_uri = os.path.join(TRAINER_PACKAGE_DIR, f'{TRAINER_PACKAGE_NAME}.tar.gz')\n",
    "\n",
    "training_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": 'n1-standard-4',\n",
    "            \"accelerator_count\": 0\n",
    "    },\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [package_uri],\n",
    "            \"python_module\": \"src.model_training.task\",\n",
    "            \"args\": trainer_args,\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2df84",
   "metadata": {},
   "source": [
    "### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97b2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a custom training job...\")\n",
    "\n",
    "job_display_name = f\"{TRAINER_PACKAGE_NAME}_{TIMESTAMP}\"\n",
    "\n",
    "custom_job = {\n",
    "            \"display_name\": job_display_name,\n",
    "            \"job_spec\": {\n",
    "                \"worker_pool_specs\": training_spec,\n",
    "                \"base_output_directory\": {\"output_uri_prefix\": EXPERIMENT_RUN_DIR},\n",
    "                \"service_account\": SERVICE_ACCOUNT,\n",
    "                \"tensorboard\": tensorboard_resource_name,\n",
    "            }\n",
    "}\n",
    "\n",
    "job_client_beta = vertex_ai_beta.JobServiceClient(\n",
    "            client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    "        )\n",
    "\n",
    "job = job_client_beta.create_custom_job(\n",
    "    parent=PARENT,\n",
    "    custom_job=custom_job\n",
    ")\n",
    "print(f\"Job {job.name} submitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571a1b7",
   "metadata": {},
   "source": [
    "### Monitor job state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88280396",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = job_client_beta.get_custom_job(name=job.name)\n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Training job completed. - Training Elapsed Time:\", response.update_time - response.create_time)\n",
    "        print(\"Training Job Time:\", response.end_time - response.start_time)\n",
    "        break\n",
    "    elif response.state.name == 'JOB_STATE_FAILED':\n",
    "        print(\"Training job failed!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Training job state is: {response.state.name}.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6595c4",
   "metadata": {},
   "source": [
    "## 5. Upload exported model to Vertex AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4380775e",
   "metadata": {},
   "source": [
    "### Generate the Explaination metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd47a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_config = features.generate_explanation_config()\n",
    "print(explanation_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a40a76e",
   "metadata": {},
   "source": [
    "### Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634937a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-4'\n",
    "SERVING_IMAGE = f\"gcr.io/cloud-aiplatform/prediction/{SERVING_RUNTIME}:latest\"\n",
    "print(SERVING_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "    inputs=explanation_config[\"inputs\"],\n",
    "    outputs=explanation_config[\"outputs\"],\n",
    ")\n",
    "explanation_parameters = vertex_ai.explain.ExplanationParameters(\n",
    "    explanation_config[\"params\"]\n",
    ")\n",
    "\n",
    "vertex_model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=SERVING_IMAGE,\n",
    "    parameters_schema_uri=None,\n",
    "    instance_schema_uri=None,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vertex_model.gca_resource)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0acea7",
   "metadata": {},
   "source": [
    "## 6. Exract Experiment Parameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dfs = vertex_ai.get_experiment_df()\n",
    "print(experiment_dfs)\n",
    "experiment_df = experiment_dfs[experiment_dfs.experiment_name == EXPERIMENT_NAME]\n",
    "print(experiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/vertex-ai/locations{REGION}/experiments/{EXPERIMENT_NAME}/metrics?project={PROJECT_ID}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1a3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
