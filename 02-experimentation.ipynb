{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e972edb5",
   "metadata": {},
   "source": [
    "# 02 - ML experimentation with a custom model\n",
    "\n",
    "The purpose of this notebook is to to train a custom `TF.keras` classifier model to predict whether a given trip will result in a tip > 20%. The notebook covers the following tasks:\n",
    "\n",
    "1. Preprocess the data locally using Apache Beam.\n",
    "2. Train and test the trained custom model locally.\n",
    "3. Submit a `Dataflow` job to preprocess the data at scale.\n",
    "4. Submit a custom training job to `Vertex AI` training service using a prebuilt container.\n",
    "5. Upload the trained model as a `Vertex Model` resource.\n",
    "6. Track experiment parameters from `Vertex ML Metadata`\n",
    "\n",
    "The notebook uses `Vertex TensorBoard` and `Vertex ML Metadata` to  track, visualize, and compare ML experiments.\n",
    "\n",
    "Learn about [custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training).\n",
    "\n",
    "Learn about [pre-built container](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers).\n",
    "\n",
    "Learn about [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview).\n",
    "\n",
    "Learn about [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13914a4c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09e243",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507c5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import aiplatform_v1beta1 as vertex_ai_beta\n",
    "\n",
    "from src.common import features, datasource_utils\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.model_training import features as feature_info\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'TensorFlow Transform: {tft.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97a22c",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-project-id]' # Change to your project id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET = '[your-bucket-name]'  # Change to your bucket name.\n",
    "SERVICE_ACCOUNT = '[your-service-account]'\n",
    "\n",
    "if PROJECT_ID == '' or PROJECT_ID is None or PROJECT_ID == '[your-project-id]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    \n",
    "if SERVICE_ACCOUNT == '' or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == '[your-service-account]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.account)' 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[0]\n",
    "    \n",
    "if BUCKET == '' or BUCKET is None or BUCKET == '[your-bucket-name]':\n",
    "    # Set your bucket name using your GCP project id\n",
    "    BUCKET = PROJECT_ID\n",
    "    # Try to create the bucket if it doesn'exists\n",
    "    ! gsutil mb -l $REGION gs://$BUCKET\n",
    "    print('')\n",
    "    \n",
    "PARENT = f'projects/{PROJECT_ID}/locations/{REGION}'\n",
    "    \n",
    "print('Project ID:', PROJECT_ID)\n",
    "print('Region:', REGION)\n",
    "print('Bucket name:', BUCKET)\n",
    "print('Service Account:', SERVICE_ACCOUNT)\n",
    "print('Vertex API Parent URI:', PARENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f043ca",
   "metadata": {},
   "source": [
    "### Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v1'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "\n",
    "WORKSPACE = f'gs://{BUCKET}/{DATASET_DISPLAY_NAME}'\n",
    "EXPERIMENT_ARTIFACTS_DIR = os.path.join(WORKSPACE, 'experiments')\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT_ID}'\n",
    "EXPERIMENT_NAME = f'{MODEL_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf19fb0",
   "metadata": {},
   "source": [
    "## Create `Vertex TensorBoard` instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a86fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT_ID} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aaf637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {TODO for Khalid, update this with SDK equivalent}\n",
    "tensorboard_client_beta = vertex_ai_beta.TensorboardServiceClient(\n",
    "    client_options={'api_endpoint': f'{REGION}-aiplatform.googleapis.com'}\n",
    ")\n",
    "\n",
    "tensorboard = [\n",
    "    resource for resource in tensorboard_client_beta.list_tensorboards(parent=PARENT) \n",
    "    if resource.display_name == TENSORBOARD_DISPLAY_NAME][0]\n",
    "\n",
    "tensorboard_resource_name = tensorboard.name\n",
    "print('TensorBoard resource name:', tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0016aa4f",
   "metadata": {},
   "source": [
    "## Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea294241",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_EXPERIMENT_ARTIFACTS = False\n",
    "\n",
    "if tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR) and REMOVE_EXPERIMENT_ARTIFACTS:\n",
    "    print('Removing previous experiment artifacts...')\n",
    "    tf.io.gfile.rmtree(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "if not tf.io.gfile.exists(EXPERIMENT_ARTIFACTS_DIR):\n",
    "    print('Creating new experiment artifacts directory...')\n",
    "    tf.io.gfile.mkdir(EXPERIMENT_ARTIFACTS_DIR)\n",
    "\n",
    "print('Workspace is ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c0cd4",
   "metadata": {},
   "source": [
    "## Initialize `Vertex Experiment` resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "run_id = f'run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}'\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print('Experiment run directory:', EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0878a",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data using Apache Beam\n",
    "\n",
    "The Apache Beam pipeline of data preprocessing is implemented in the [preprocessing](src/preprocessing) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6c12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51aab02",
   "metadata": {},
   "source": [
    "### Get source query from `Dataset` resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2486d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "raw_data_query = datasource_utils.create_bq_source_query(\n",
    "    dataset_display_name=DATASET_DISPLAY_NAME,\n",
    "    missing=feature_info.MISSING_VALUES,\n",
    "    label_column=feature_info.TARGET_FEATURE_NAME,\n",
    "    ML_use=ML_USE,\n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5b8fb",
   "metadata": {},
   "source": [
    "### Test data preprocessing locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd586853",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'{BUCKET}/bq_tmp',\n",
    "    'project': PROJECT_ID\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d492bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data preprocessing started...')\n",
    "etl.run_transform_pipeline(args)\n",
    "print('Data preprocessing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd79c66",
   "metadata": {},
   "source": [
    "## 2. Train a custom `TF.Keras` model locally\n",
    "\n",
    "The `TF.Keras` implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929783d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2edd908",
   "metadata": {},
   "source": [
    "### Read the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7840155",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'eval/data-*.gz')\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f'{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}')\n",
    "    print(f'target: {target.numpy().tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6465b9e",
   "metadata": {},
   "source": [
    "### Update the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'hidden_units': [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a335c0",
   "metadata": {},
   "source": [
    "### Create and test model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d021f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.create_binary_classifier(tft_output, hyperparams)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d5ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94baa720",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1957b258",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b54a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams['learning_rate'] = 0.001\n",
    "hyperparams['num_epochs'] = 5\n",
    "hyperparams['batch_size'] = 512\n",
    "\n",
    "vertex_ai.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27807f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(\n",
    "    model=classifier,\n",
    "    data_dir=eval_data_file_pattern,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96857e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_metrics(\n",
    "    {'val_loss': val_loss, 'val_accuracy': val_accuracy}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ee791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (for Khalid, this output has an ignored exception. Can we eliminate the exception?)\n",
    "\n",
    "! tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e00c7d",
   "metadata": {},
   "source": [
    "### Export the trained model in SavedModel format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTIFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84424a4",
   "metadata": {},
   "source": [
    "### Inspect model serving signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbbe3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe04c28",
   "metadata": {},
   "source": [
    "### Test the exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebfd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ffcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '/data-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f'{key}: {predictions[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b113462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    'dropoff_grid': 'POINT(-87.6 41.9)',\n",
    "    'euclidean': 2064.2696,\n",
    "    'loc_cross': '',\n",
    "    'payment_type': 'Credit Card',\n",
    "    'pickup_grid': 'POINT(-87.6 41.9)',\n",
    "    'trip_miles': 1.37,\n",
    "    'trip_day': 12,\n",
    "    'trip_hour': 6,\n",
    "    'trip_month': 2,\n",
    "    'trip_day_of_week': 4,\n",
    "    'trip_seconds': 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04452c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f'{key}: {predictions[key].numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17916f3",
   "metadata": {},
   "source": [
    "## Start a new `Vertex Experiment` run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=BUCKET,\n",
    "    experiment=EXPERIMENT_NAME)\n",
    "\n",
    "run_id = f'run-gcp-{datetime.now().strftime('%Y%m%d%H%M%S')}'\n",
    "vertex_ai.start_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(EXPERIMENT_ARTIFACTS_DIR, EXPERIMENT_NAME, run_id)\n",
    "print('Experiment run directory:', EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9e022",
   "metadata": {},
   "source": [
    "## 3. Submit a data processing job to `Dataflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca7892",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef08107",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_USE = 'UNASSIGNED'\n",
    "LIMIT = 1000000\n",
    "\n",
    "raw_data_query = datasource_utils.create_bq_source_query(\n",
    "    dataset_display_name=DATASET_DISPLAY_NAME,\n",
    "    missing=feature_info.MISSING_VALUES,\n",
    "    label_column='tip_bin',\n",
    "    ML_use=ML_USE,\n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'runner': 'DataflowRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artifact_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT_ID,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ae46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258eccad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print('Data preprocessing started...')\n",
    "etl.run_transform_pipeline(args)\n",
    "print('Data preprocessing completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ac6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0254683",
   "metadata": {},
   "source": [
    "## 4. Submit a custom training job to `Vertex AI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83db3f",
   "metadata": {},
   "source": [
    "### Test the training task locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e3f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTIFACTS_DIR} \\\n",
    "    --num-epochs=5 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e25d0d9",
   "metadata": {},
   "source": [
    "### Prepare the training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b092b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')\n",
    "TRAINER_PACKAGE_NAME = f'{MODEL_DISPLAY_NAME}_trainer'\n",
    "print('Trainer package upload location:', TRAINER_PACKAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1960448",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/__pycache__/\n",
    "!rm -r src/.ipynb_checkpoints/\n",
    "!rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}.tar.gz\n",
    "\n",
    "!mkdir {TRAINER_PACKAGE_NAME}\n",
    "\n",
    "!cp setup.py {TRAINER_PACKAGE_NAME}/\n",
    "!cp -r src {TRAINER_PACKAGE_NAME}/\n",
    "!tar cvf {TRAINER_PACKAGE_NAME}.tar {TRAINER_PACKAGE_NAME}\n",
    "!gzip {TRAINER_PACKAGE_NAME}.tar\n",
    "!gsutil cp {TRAINER_PACKAGE_NAME}.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r {TRAINER_PACKAGE_NAME}\n",
    "!rm -r {TRAINER_PACKAGE_NAME}.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4092c3",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RUNTIME = 'tf-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/cloud-aiplatform/training/{TRAIN_RUNTIME}:latest'\n",
    "print('Training image:', TRAIN_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_units ='64,64'\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX}/train/*',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTIFACTS_DIR}',\n",
    "    f'--num-epochs={num_epochs}',\n",
    "    f'--learning-rate={learning_rate}',\n",
    "    f'--project={PROJECT_ID}',\n",
    "    f'--region={REGION}',\n",
    "    f'--staging-bucket={BUCKET}',\n",
    "    f'--experiment-name={EXPERIMENT_NAME}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40ca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_uri = os.path.join(TRAINER_PACKAGE_DIR, f'{TRAINER_PACKAGE_NAME}.tar.gz')\n",
    "\n",
    "training_spec = [\n",
    "    {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-4',\n",
    "            'accelerator_count': 0\n",
    "    },\n",
    "        'python_package_spec': {\n",
    "            'executor_image_uri': TRAIN_IMAGE,\n",
    "            'package_uris': [package_uri],\n",
    "            'python_module': 'src.model_training.task',\n",
    "            'args': trainer_args,\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d116817f",
   "metadata": {},
   "source": [
    "### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909384f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Submitting a custom training job...')\n",
    "\n",
    "job_display_name = f'{TRAINER_PACKAGE_NAME}_{datetime.now().strftime('%Y%m%d%H%M%S')}'\n",
    "\n",
    "custom_job = {\n",
    "    'display_name': job_display_name,\n",
    "    'job_spec': {\n",
    "        'worker_pool_specs': training_spec,\n",
    "        'base_output_directory': {'output_uri_prefix': EXPERIMENT_RUN_DIR},\n",
    "        'service_account': SERVICE_ACCOUNT,\n",
    "        'tensorboard': tensorboard_resource_name,\n",
    "    }\n",
    "}\n",
    "\n",
    "job_client_beta = vertex_ai_beta.JobServiceClient(\n",
    "    client_options={'api_endpoint': f'{REGION}-aiplatform.googleapis.com'}\n",
    ")\n",
    "\n",
    "job = job_client_beta.create_custom_job(\n",
    "    parent=PARENT,\n",
    "    custom_job=custom_job\n",
    ")\n",
    "\n",
    "print(f'Job {job.name} submitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e182d6",
   "metadata": {},
   "source": [
    "### Monitor job state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcbd77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = job_client_beta.get_custom_job(name=job.name)\n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print('Training job completed. - Training Elapsed Time:', response.update_time - response.create_time)\n",
    "        print('Training Job Time:', response.end_time - response.start_time)\n",
    "        break\n",
    "    elif response.state.name == 'JOB_STATE_FAILED':\n",
    "        print('Training job failed!')\n",
    "        break\n",
    "    else:\n",
    "        print(f'Training job state is: {response.state.name}.')\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0a61c",
   "metadata": {},
   "source": [
    "## 5. Upload exported model to Vertex AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9183b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls {EXPORT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7301cd4b",
   "metadata": {},
   "source": [
    "### Generate the Explaination metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc5574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO { for Khalid, there should be support in Vertex SDK to auto-gen the explain config}\n",
    "# Alternatively, generalize the function and move the Chicago specific stuff into the notebook\n",
    "explanation_config = features.generate_explanation_config()\n",
    "explanation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f721254",
   "metadata": {},
   "source": [
    "### Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e3964",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-4'\n",
    "SERVING_IMAGE = f'gcr.io/cloud-aiplatform/prediction/{SERVING_RUNTIME}:latest'\n",
    "print('Serving image:', SERVING_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a39582",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_metadata = vertex_ai.explain.ExplanationMetadata(\n",
    "    inputs=explanation_config['inputs'],\n",
    "    outputs=explanation_config['outputs'],\n",
    ")\n",
    "explanation_parameters = vertex_ai.explain.ExplanationParameters(\n",
    "    explanation_config['params']\n",
    ")\n",
    "\n",
    "vertex_model = vertex_ai.Model.upload(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    artifact_uri=EXPORT_DIR,\n",
    "    serving_container_image_uri=SERVING_IMAGE,\n",
    "    parameters_schema_uri=None,\n",
    "    instance_schema_uri=None,\n",
    "    explanation_metadata=explanation_metadata,\n",
    "    explanation_parameters=explanation_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model.gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7bd93",
   "metadata": {},
   "source": [
    "## 6. Exract experiment run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = vertex_ai.get_experiment_df()\n",
    "experiment_df = experiment_df[experiment_df.experiment_name == EXPERIMENT_NAME]\n",
    "experiment_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b27775",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vertex AI Experiments:')\n",
    "print(\n",
    "    f'https://console.cloud.google.com/vertex-ai/locations{REGION}/experiments/{EXPERIMENT_NAME}/metrics?project={PROJECT_ID}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e7b541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
