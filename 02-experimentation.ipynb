{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funded-apollo",
   "metadata": {},
   "source": [
    "# 02 - ML Experimentation with Custom Model\n",
    "\n",
    "The purpose of this notebook is to use [custom training](https://cloud.google.com/ai-platform-unified/docs/training/custom-training) to train a keras classifier to predict whether a given trip will result in a tip > 20%. The notebook covers the following tasks:\n",
    "1. Preprocess the data locally using Apache Bean.\n",
    "2. Train and test custom model locally using a Keras implementation.\n",
    "3. Submit a Dataflow job to preprocess the data at scale.\n",
    "4. Submit a custom training job to Vertex AI using a [pre-built container](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "5. Upload the trained model to Vertex AI.\n",
    "6. Exract and visualize experiment parameters from [Vertex AI Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction).\n",
    "\n",
    "We use [Vertex TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-overview) \n",
    "and [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction) to  track, visualize, and compare ML experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-tribute",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-audience",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from src.common import features\n",
    "from src.model_training import data, model, defaults, trainer, exporter\n",
    "from src.utils import datasource_utils\n",
    "from src.preprocessing import etl\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"TensorFlow Transform: {tft.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'  # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us'  # Change to your bucket.\n",
    "SERVICE_ACCOUNT = \"sa-vertex-ai-mlops@ksalama-cloudml.iam.gserviceaccount.com\"\n",
    "\n",
    "DATASET_DISPLAY_NAME = 'chicago_taxi_tips_v1'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}_classifier_v1'\n",
    "\n",
    "WORKSPACE = f'gs://{BUCKET}/ucaip_demo/chicago_taxi/experiments'\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'\n",
    "\n",
    "TENSORBOARD_DISPLAY_NAME = f'tb-{PROJECT}'\n",
    "EXPERIMENT_NAME = f'{DATASET_DISPLAY_NAME}-experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.vertex_utils import VertexClient\n",
    "vertex_client = VertexClient(PROJECT, REGION, BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-theme",
   "metadata": {},
   "source": [
    "## Create Vertex TensorBoard Instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud beta ai tensorboards create --display-name={TENSORBOARD_DISPLAY_NAME} \\\n",
    "  --project={PROJECT} --region={REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_resource_name = vertex_client.get_tensorboard_by_display_name(TENSORBOARD_DISPLAY_NAME).name\n",
    "print(\"TensorBoard resource name:\", tensorboard_resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-somalia",
   "metadata": {},
   "source": [
    "## Initialize Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_WORKSPACE = False\n",
    "if tf.io.gfile.exists(WORKSPACE) and REMOVE_WORKSPACE:\n",
    "    print(\"Removing previous local workspace...\")\n",
    "    tf.io.gfile.rmtree(WORKSPACE)\n",
    "\n",
    "if not tf.io.gfile.exists(WORKSPACE):\n",
    "    print(\"Creating new local workspace...\")\n",
    "    tf.io.gfile.mkdir(WORKSPACE)\n",
    "\n",
    "print(\"Workspace is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-seating",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.set_experiment(EXPERIMENT_NAME)\n",
    "run_id = f\"run-local-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "_ = vertex_client.start_experiment_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(WORKSPACE, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-gospel",
   "metadata": {},
   "source": [
    "## 1. Preprocess the data using Apache Beam\n",
    "\n",
    "The Apache Beam pipeline of data preprocessing is implemented in the [preprocessing](src/preprocessing) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-reasoning",
   "metadata": {},
   "source": [
    "### Get Source Query from Managed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'UNASSIGNED'\n",
    "LIMIT = 5120\n",
    "\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    data_split=DATA_SPLIT, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-threat",
   "metadata": {},
   "source": [
    "### Test Data Preprocessing Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artefact_dir': TRANSFORM_ARTEFACTS_DIR,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': f'gs://{BUCKET}/bq_tmp',\n",
    "    'project': PROJECT\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-bermuda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.log_params({\"transformed_data_path\": TRANSFORMED_DATA_PREFIX})\n",
    "vertex_client.log_params({\"transform_artifacts_dir\": TRANSFORM_ARTEFACTS_DIR})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-adaptation",
   "metadata": {},
   "source": [
    "## 2. Train a Custom Model Localy using a Keras Implementation\n",
    "\n",
    "The Keras implementation of the custom model is in the [model_training](src/model_training) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-northwest",
   "metadata": {},
   "source": [
    "### Read transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTEFACTS_DIR)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "transform_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'train/data-*.gz')\n",
    "eval_data_file_pattern = os.path.join(TRANSFORMED_DATA_PREFIX,'eval/data-*.gz')\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-serum",
   "metadata": {},
   "source": [
    "### Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layers = model.create_model_inputs()\n",
    "input_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-highlight",
   "metadata": {},
   "source": [
    "### Create hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"hidden_units\": [64, 32]\n",
    "}\n",
    "\n",
    "hyperparams = defaults.update_hyperparams(hyperparams)\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-lyric",
   "metadata": {},
   "source": [
    "### Create and test model inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model.create_binary_classifier(tft_output, hyperparams)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(\n",
    "    classifier, \n",
    "    show_shapes=True, \n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-evans",
   "metadata": {},
   "source": [
    "### Train the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 5\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "vertex_client.log_params(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = trainer.train(\n",
    "    train_data_dir=train_data_file_pattern,\n",
    "    eval_data_dir=eval_data_file_pattern,\n",
    "    tft_output_dir=TRANSFORM_ARTEFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    "    log_dir=LOG_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_accuracy = trainer.evaluate(\n",
    "    model=classifier,\n",
    "    data_dir=eval_data_file_pattern,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTEFACTS_DIR,\n",
    "    hyperparams=hyperparams,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.log_metrics(\n",
    "    {\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tb-gcp-uploader --tensorboard_resource_name={tensorboard_resource_name} \\\n",
    "  --logdir={LOG_DIR} \\\n",
    "  --experiment_name={EXPERIMENT_NAME} --one_shot=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-cooking",
   "metadata": {},
   "source": [
    "### Export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-preview",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = os.path.join(EXPORT_DIR)\n",
    "\n",
    "exporter.export_serving_model(\n",
    "    classifier=classifier,\n",
    "    serving_model_dir=saved_model_dir,\n",
    "    raw_schema_location=RAW_SCHEMA_LOCATION,\n",
    "    tft_output_dir=TRANSFORM_ARTEFACTS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir={saved_model_dir} --tag_set=serve --signature_def=serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-might",
   "metadata": {},
   "source": [
    "### Test the exported SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_model = tf.saved_model.load(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-floating",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = tf.data.TFRecordDataset.list_files(EXPORTED_DATA_PREFIX + '-*.tfrecord')\n",
    "for batch in tf.data.TFRecordDataset(file_names).batch(3).take(1):\n",
    "    predictions = serving_model.signatures['serving_tf_example'](batch)\n",
    "    for key in predictions:\n",
    "        print(f\"{key}: {predictions[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "raw_schema = tfdv.load_schema_text(RAW_SCHEMA_LOCATION)\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(raw_schema).feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"dropoff_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"euclidean\": 2064.2696,\n",
    "    \"loc_cross\": \"\",\n",
    "    \"payment_type\": \"Credit Card\",\n",
    "    \"pickup_grid\": \"POINT(-87.6 41.9)\",\n",
    "    \"trip_miles\": 1.37,\n",
    "    \"trip_day\": 12,\n",
    "    \"trip_hour\": 6,\n",
    "    \"trip_month\": 2,\n",
    "    \"trip_day_of_week\": 4,\n",
    "    \"trip_seconds\": 555,\n",
    "}\n",
    "\n",
    "for feature_name in instance:\n",
    "    dtype = raw_feature_spec[feature_name].dtype\n",
    "    instance[feature_name] = tf.constant([[instance[feature_name]]], dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-determination",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = serving_model.signatures['serving_default'](**instance)\n",
    "for key in predictions:\n",
    "    print(f\"{key}: {predictions[key].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-neighborhood",
   "metadata": {},
   "source": [
    "## Start a new Vertex AI Experiment Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.set_experiment(EXPERIMENT_NAME)\n",
    "run_id = f\"run-gcp-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "_ = vertex_client.start_experiment_run(run_id)\n",
    "\n",
    "EXPERIMENT_RUN_DIR = os.path.join(WORKSPACE, EXPERIMENT_NAME, run_id)\n",
    "print(\"Experiment run directory:\", EXPERIMENT_RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-christopher",
   "metadata": {},
   "source": [
    "## 3. Submit a Data Processing Job to Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORTED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'exported_data')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(EXPERIMENT_RUN_DIR, 'transformed_data')\n",
    "TRANSFORM_ARTEFACTS_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'transform_artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'UNASSIGNED'\n",
    "LIMIT = 1000000\n",
    "raw_data_query = datasource_utils.get_training_source_query(\n",
    "    project=PROJECT, \n",
    "    region=REGION, \n",
    "    dataset_display_name=DATASET_DISPLAY_NAME, \n",
    "    data_split=DATA_SPLIT, \n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "args = {\n",
    "    'runner': 'DataflowRunner',\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "    'transform_artefact_dir': TRANSFORM_ARTEFACTS_DIR,\n",
    "    'write_raw_data': False,\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-lawrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.log_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "print(\"Data preprocessing started...\")\n",
    "etl.run_transform_pipeline(args)\n",
    "print(\"Data preprocessing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {EXPERIMENT_RUN_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_client.log_params({\"transformed_data_path\": TRANSFORMED_DATA_PREFIX})\n",
    "vertex_client.log_params({\"transform_artifacts_dir\": TRANSFORM_ARTEFACTS_DIR})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-concern",
   "metadata": {},
   "source": [
    "## 4. Submit a Custom Training Job to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'logs')\n",
    "EXPORT_DIR = os.path.join(EXPERIMENT_RUN_DIR, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-swimming",
   "metadata": {},
   "source": [
    "### Test the training task locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.model_training.task \\\n",
    "    --model-dir={EXPORT_DIR} \\\n",
    "    --log-dir={LOG_DIR} \\\n",
    "    --train-data-dir={TRANSFORMED_DATA_PREFIX}/train/* \\\n",
    "    --eval-data-dir={TRANSFORMED_DATA_PREFIX}/eval/*  \\\n",
    "    --tft-output-dir={TRANSFORM_ARTEFACTS_DIR} \\\n",
    "    --num-epochs=5 \\\n",
    "    --hidden-units=32,32 \\\n",
    "    --experiment-name={EXPERIMENT_NAME} \\\n",
    "    --run-name={run_id} \\\n",
    "    --project={PROJECT} \\\n",
    "    --region={REGION} \\\n",
    "    --staging-bucket={BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-final",
   "metadata": {},
   "source": [
    "### Prepare training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINER_PACKAGE_DIR = os.path.join(WORKSPACE, 'trainer_packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/__pycache__/\n",
    "!rm -r src/.ipynb_checkpoints/\n",
    "!rm -r src/raw_schema/.ipynb_checkpoints/\n",
    "!rm -f custom_job.tar custom_job.tar.gz\n",
    "\n",
    "!mkdir custom_job\n",
    "\n",
    "!cp setup.py custom_job/\n",
    "!cp -r src custom_job/\n",
    "!tar cvf custom_job.tar custom_job\n",
    "!gzip custom_job.tar\n",
    "!gsutil cp custom_job.tar.gz {TRAINER_PACKAGE_DIR}/\n",
    "!rm -r custom_job\n",
    "!rm -r custom_job.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-novel",
   "metadata": {},
   "source": [
    "### Prepare the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RUNTIME = 'tf-cpu.2-4'\n",
    "TRAIN_IMAGE = f\"gcr.io/cloud-aiplatform/training/{TRAIN_RUNTIME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "hidden_units = \"64,64\"\n",
    "\n",
    "trainer_args = [\n",
    "    f'--train-data-dir={TRANSFORMED_DATA_PREFIX + \"/train/*\"}',\n",
    "    f'--eval-data-dir={TRANSFORMED_DATA_PREFIX + \"/eval/*\"}',\n",
    "    f'--tft-output-dir={TRANSFORM_ARTEFACTS_DIR}',\n",
    "    f'--num-epochs={num_epochs}',\n",
    "    f'--learning-rate={learning_rate}',\n",
    "    f'--project={PROJECT}',\n",
    "    f'--region={REGION}',\n",
    "    f'--staging-bucket={BUCKET}',\n",
    "    f'--experiment-name={EXPERIMENT_NAME}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spec = [\n",
    "    {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": 'n1-standard-4',\n",
    "            \"accelerator_count\": 0\n",
    "    },\n",
    "        \"python_package_spec\": {\n",
    "            \"executor_image_uri\": TRAIN_IMAGE,\n",
    "            \"package_uris\": [os.path.join(TRAINER_PACKAGE_DIR, 'custom_job.tar.gz')],\n",
    "            \"python_module\": \"src.model_training.task\",\n",
    "            \"args\": trainer_args,\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-electricity",
   "metadata": {},
   "source": [
    "### Submit the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a custom training job...\")\n",
    "job = vertex_client.submit_custom_job(\n",
    "    training_spec=training_spec,\n",
    "    experiment_dir=EXPERIMENT_RUN_DIR,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard_resource_name=tensorboard_resource_name\n",
    ")\n",
    "print(f\"Job {job.name} sbumitted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-enhancement",
   "metadata": {},
   "source": [
    "### Monitor job state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = vertex_client.get_custom_job_by_uri(job.name)\n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Training job completed. - Training Time:\", response.update_time - response.create_time)\n",
    "        break\n",
    "    elif response.state.name == 'JOB_STATE_FAILED':\n",
    "        print(\"Training job failed!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Training job state is: {response.state.name}.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-nelson",
   "metadata": {},
   "source": [
    "## 5. Upload exported model to Vertex AI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model_dir = os.path.join(TRAINING_DIR, 'model')\n",
    "!gsutil ls {exported_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_RUNTIME='tf2-cpu.2-4'\n",
    "SERVING_IMAGE = f\"gcr.io/cloud-aiplatform/prediction/{SERVING_RUNTIME}:latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model = vertex_client.upload_model(\n",
    "    display_name=MODEL_DISPLAY_NAME,\n",
    "    model_artifact_uri=exported_model_dir,\n",
    "    serving_image_uri=SERVING_IMAGE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-window",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model.gca_resource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-click",
   "metadata": {},
   "source": [
    "## 6. Exract and Visualize Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-guarantee",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_df = vertex_client.get_experiment_df(EXPERIMENT_NAME)\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "\n",
    "ax = pd.plotting.parallel_coordinates(\n",
    "    experiment_df.reset_index(level=0),\n",
    "    \"run_name\",\n",
    "    cols=[\n",
    "        \"param.num_epochs\",\n",
    "        \"param.hidden_units\",\n",
    "        \"param.learning_rate\",\n",
    "        \"metric.val_loss\",\n",
    "        \"metric.val_accuraccy\",\n",
    "    ],\n",
    ")\n",
    "ax.set_yscale(\"symlog\")\n",
    "ax.legend(bbox_to_anchor=(1.0, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vertex AI Experiments:\")\n",
    "print(\n",
    "    f\"https://console.cloud.google.com/vertex-ai/locations{REGION}/experiments/{EXPERIMENT_NAME}/metrics?project={PROJECT}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-exclusion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
