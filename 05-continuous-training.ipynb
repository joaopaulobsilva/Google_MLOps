{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "finite-basket",
   "metadata": {},
   "source": [
    "# 05 - Continuous Training\n",
    "\n",
    "After testing, compiling, and uploading the pipeline definition to Cloud Storage, the pipeline is executed with respect to a trigger. We use [Cloud Functions](https://cloud.google.com/functions) and [Cloud Pub/Sub](https://cloud.google.com/pubsub) as a triggering mechanism. The triggering can be scheduled using [Cloud Schedular](https://cloud.google.com/scheduler). The trigger source sends a message to a Cloud Pub/Sub topic that the Cloud Function listens to, and then it submits the pipeline to AI Platform Managed Pipelines to be executed.\n",
    "\n",
    "This notebook covers the following steps:\n",
    "1. Create the Cloud Pub/Sub topic.\n",
    "2. Deploy the Cloud Function \n",
    "3. Test triggering a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-garlic",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tfx\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml' # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us' # Change to your bucket.\n",
    "\n",
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago-taxi-tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}-classifier-{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "PIPELINES_STORE = f'gs://{BUCKET}/ucaip_demo/compiled_pipelines/'\n",
    "GCS_PIPELINE_FILE_LOCATION = os.path.join(PIPELINES_STORE, f'{PIPELINE_NAME}.json')\n",
    "PUBSUB_TOPIC = f'trigger-{PIPELINE_NAME}'\n",
    "CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {GCS_PIPELINE_FILE_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-estate",
   "metadata": {},
   "source": [
    "## 1. Create a Pub/Sub topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create {PUBSUB_TOPIC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-provider",
   "metadata": {},
   "source": [
    "## 2. Deploy the Cloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dash separated pipeline parameter names\n",
    "PARAMETER_NAMES='num_epochs-hidden_units-learning_rate-batch_size'\n",
    "\n",
    "ENV_VARS=f\"\"\"\\\n",
    "PROJECT={PROJECT},\\\n",
    "REGION={REGION},\\\n",
    "GCS_PIPELINE_FILE_LOCATION={GCS_PIPELINE_FILE_LOCATION},\\\n",
    "PARAMETER_NAMES={PARAMETER_NAMES}\n",
    "\"\"\"\n",
    "\n",
    "!echo {ENV_VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/pipeline_triggering/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions deploy {CLOUD_FUNCTION_NAME} \\\n",
    "    --region={REGION} \\\n",
    "    --trigger-topic={PUBSUB_TOPIC} \\\n",
    "    --runtime=python37 \\\n",
    "    --source=src/pipeline_triggering\\\n",
    "    --entry-point=trigger_pipeline\\\n",
    "    --stage-bucket={BUCKET}\\\n",
    "    --update-env-vars={ENV_VARS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-plant",
   "metadata": {},
   "source": [
    "## 3. Test Triggering the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub\n",
    "\n",
    "publish_client = pubsub.PublisherClient()\n",
    "topic = f'projects/{PROJECT}/topics/{PUBSUB_TOPIC}'\n",
    "data = {\n",
    "    'source_uri': 'pubsub/function/pipline',\n",
    "    'num_epochs': 7,\n",
    "    'learning_rate': 0.003,\n",
    "    'batch_size': 512,\n",
    "    'hidden_units': '256,126'\n",
    "}\n",
    "message = json.dumps(data)\n",
    "\n",
    "_ = publish_client.publish(topic, message.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-renewal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
