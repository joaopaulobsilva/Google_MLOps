{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polar-gabriel",
   "metadata": {},
   "source": [
    "# 05 - Continuous Training\n",
    "\n",
    "After testing, compiling, and uploading the pipeline definition to Cloud Storage, the pipeline is executed with respect to a trigger. We use [Cloud Functions](https://cloud.google.com/functions) and [Cloud Pub/Sub](https://cloud.google.com/pubsub) as a triggering mechanism. The triggering can be scheduled using [Cloud Schedular](https://cloud.google.com/scheduler). The trigger source sends a message to a Cloud Pub/Sub topic that the Cloud Function listens to, and then it submits the pipeline to AI Platform Managed Pipelines to be executed.\n",
    "\n",
    "This notebook covers the following steps:\n",
    "1. Create the Cloud Pub/Sub topic.\n",
    "2. Deploy the Cloud Function \n",
    "3. Test triggering a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-submission",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tfx\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml' # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us' # Change to your bucket.\n",
    "\n",
    "PIPELINE_NAME = 'chicago-taxi-tips-classifier-v01-train-pipeline'\n",
    "PIPELINES_STORE = f'gs://{BUCKET}/ucaip_demo/compiled_pipelines/'\n",
    "GCS_PIPELINE_FILE_LOCATION = os.path.join(PIPELINES_STORE, f'{PIPELINE_NAME}.json')\n",
    "PUBSUB_TOPIC = f'trigger-{PIPELINE_NAME}'\n",
    "CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-improvement",
   "metadata": {},
   "source": [
    "## (Optional) Create a Dummy Pipeline for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_PIPELINE_ROOT = f\"gs://{BUCKET}/ucaip_demo/dummy/pipelines\"\n",
    "PIPELINE_NAME = 'dummy-pipeline'\n",
    "PARAMETER_NAMES = 'file_uri'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-subscription",
   "metadata": {},
   "source": [
    "### Implement the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.dsl.components.common.importer import Importer\n",
    "from tfx.types.experimental.simple_artifacts import File\n",
    "from tfx.orchestration import data_types\n",
    "\n",
    "def create_dummy_pipeline(\n",
    "    pipeline_root,\n",
    "    file_uri\n",
    "):\n",
    "    importer = Importer(\n",
    "        source_uri=file_uri,\n",
    "        artifact_type=File\n",
    "    ).with_id(\"DummyImporterStep\")\n",
    "    \n",
    "    return tfx.orchestration.pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[importer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-leone",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "\n",
    "dummy_pipeline_definition_file = f'{PIPELINE_NAME}.json'\n",
    "\n",
    "dummy_pipeline = create_dummy_pipeline(\n",
    "    pipeline_root=DUMMY_PIPELINE_ROOT,\n",
    "    file_uri=data_types.RuntimeParameter(\n",
    "        name='file_uri',\n",
    "        default='path/to/default/dummy.txt',\n",
    "        ptype=str,\n",
    "    )\n",
    ")\n",
    "\n",
    "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=dummy_pipeline_definition_file\n",
    ")\n",
    "    \n",
    "runner.run(dummy_pipeline, write_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-proposition",
   "metadata": {},
   "source": [
    "### Upload pipeline to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PIPELINE_FILE_LOCATION = f'gs://{BUCKET}/ucaip_demo/compiled_pipelines/{PIPELINE_NAME}.json'\n",
    "!gsutil cp {PIPELINE_NAME}.json {GCS_PIPELINE_FILE_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-socket",
   "metadata": {},
   "source": [
    "### Trigger the pipeline on Vertex AI Managed Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline_triggering import main\n",
    "import base64\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['GCS_PIPELINE_FILE_LOCATION'] = GCS_PIPELINE_FILE_LOCATION\n",
    "os.environ['PARAMETER_NAMES'] = PARAMETER_NAMES\n",
    "\n",
    "parameters = {\n",
    "    'file_uri': 'path/to/trigger/trigger/dummy.txt',\n",
    "    'unused_param': 0}\n",
    "\n",
    "message = base64.b64encode(json.dumps(parameters).encode())\n",
    "main.trigger_pipeline(\n",
    "    event={'data': message},\n",
    "    context=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-bulgaria",
   "metadata": {},
   "source": [
    "## 1. Create a Pub/Sub topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create {PUBSUB_TOPIC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-model",
   "metadata": {},
   "source": [
    "## 2. Deploy the Cloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dash separated pipeline parameter names\n",
    "PARAMETER_NAMES='num_epochs-hidden_units-learning_rate-batch_size'\n",
    "\n",
    "ENV_VARS=f\"\"\"\\\n",
    "PROJECT={PROJECT},\\\n",
    "REGION={REGION},\\\n",
    "GCS_PIPELINE_FILE_LOCATION={GCS_PIPELINE_FILE_LOCATION},\\\n",
    "PARAMETER_NAMES={PARAMETER_NAMES}\n",
    "\"\"\"\n",
    "\n",
    "!echo {ENV_VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/pipeline_triggering/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions deploy {CLOUD_FUNCTION_NAME} \\\n",
    "    --region={REGION} \\\n",
    "    --trigger-topic={PUBSUB_TOPIC} \\\n",
    "    --runtime=python37 \\\n",
    "    --source=src/pipeline_triggering\\\n",
    "    --entry-point=trigger_pipeline\\\n",
    "    --stage-bucket={BUCKET}\\\n",
    "    --update-env-vars={ENV_VARS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-reverse",
   "metadata": {},
   "source": [
    "## 3. Test Triggering the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub\n",
    "\n",
    "publish_client = pubsub.PublisherClient()\n",
    "topic = f'projects/{PROJECT}/topics/{PUBSUB_TOPIC}'\n",
    "data = {\n",
    "    'source_uri': 'pubsub/function/pipline',\n",
    "    'num_epochs': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 512,\n",
    "    'hidden_units': '256,126'\n",
    "}\n",
    "message = json.dumps(data)\n",
    "\n",
    "_ = publish_client.publish(topic, message.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
