{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d16278",
   "metadata": {},
   "source": [
    "# 05 - Continuous Training\n",
    "\n",
    "After testing, compiling, and uploading the pipeline definition to Cloud Storage, the pipeline is executed with respect to a trigger. We use [Cloud Functions](https://cloud.google.com/functions) and [Cloud Pub/Sub](https://cloud.google.com/pubsub) as a triggering mechanism. The triggering can be scheduled using [Cloud Schedular](https://cloud.google.com/scheduler). The trigger source sends a message to a Cloud Pub/Sub topic that the Cloud Function listens to, and then it submits the pipeline to AI Platform Managed Pipelines to be executed.\n",
    "\n",
    "This notebook covers the following steps:\n",
    "1. Create the Cloud Pub/Sub topic.\n",
    "2. Deploy the Cloud Function \n",
    "3. Test triggering a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f92f86",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5361ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a5e7f",
   "metadata": {},
   "source": [
    "Install the latest GA version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58119ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb99e51",
   "metadata": {},
   "source": [
    "Install deep learning dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1cd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U tfx==0.30.0 $USER_FLAG\n",
    "! pip3 install -r requirements.txt $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381eac3",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the Vertex SDK and Google *cloud-storage*, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7bd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6a32f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tfx\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4f5888",
   "metadata": {},
   "source": [
    "### Setup your Google Cloud project\n",
    "\n",
    "Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b14e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aed5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)\n",
    "    \n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c275c",
   "metadata": {},
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex. Not all regions provide support for all Vertex services. For the latest support per region, see the [Vertex locations documentation](https://cloud.google.com/ai-platform-unified/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a094d2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e1d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978b2e6",
   "metadata": {},
   "source": [
    "### Setup a bucket for continuous training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a0502",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24c2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"_vertex-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd547a",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44654c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdbb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'v01'\n",
    "DATASET_DISPLAY_NAME = 'chicago_taxi_tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}_classifier_{VERSION}'\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}-train-pipeline'\n",
    "\n",
    "PIPELINES_STORE = f'{BUCKET_NAME}/vertex_demo/compiled_pipelines/'\n",
    "GCS_PIPELINE_FILE_LOCATION = os.path.join(PIPELINES_STORE, f'{PIPELINE_NAME}.json')\n",
    "PUBSUB_TOPIC = f'trigger-{PIPELINE_NAME}'\n",
    "CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd5a139",
   "metadata": {},
   "source": [
    "## (Optional) Create a dummy pipeline for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc281916",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_PIPELINE_ROOT = f\"{BUCKET_NAME}/vertex_demo/dummy/pipelines\"\n",
    "PIPELINE_NAME = 'dummy-pipeline'\n",
    "PARAMETER_NAMES = 'file_uri'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86a9ff",
   "metadata": {},
   "source": [
    "### Implement the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.dsl.components.common.importer import Importer\n",
    "from tfx.types.experimental.simple_artifacts import File\n",
    "from tfx.orchestration import data_types\n",
    "\n",
    "\n",
    "def create_dummy_pipeline(\n",
    "    pipeline_root,\n",
    "    file_uri\n",
    "):\n",
    "    importer = Importer(\n",
    "        source_uri=file_uri,\n",
    "        artifact_type=File\n",
    "    ).with_id(\"DummyImporterStep\")\n",
    "    \n",
    "    return tfx.orchestration.pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[importer]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557c588",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "\n",
    "dummy_pipeline_definition_file = f'{PIPELINE_NAME}.json'\n",
    "\n",
    "dummy_pipeline = create_dummy_pipeline(\n",
    "    pipeline_root=DUMMY_PIPELINE_ROOT,\n",
    "    file_uri=data_types.RuntimeParameter(\n",
    "        name='file_uri',\n",
    "        default='path/to/default/dummy.txt',\n",
    "        ptype=str,\n",
    "    )\n",
    ")\n",
    "\n",
    "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=dummy_pipeline_definition_file\n",
    ")\n",
    "    \n",
    "runner.run(dummy_pipeline, write_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d706338",
   "metadata": {},
   "source": [
    "### Upload pipeline to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b9ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PIPELINE_FILE_LOCATION = f'{BUCKET_NAME}/vertex_demo/compiled_pipelines/{PIPELINE_NAME}.json'\n",
    "! gsutil cp {PIPELINE_NAME}.json {GCS_PIPELINE_FILE_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3f97c",
   "metadata": {},
   "source": [
    "### Trigger the pipeline on Vertex AI Managed Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipeline_triggering import main\n",
    "import base64\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT_ID\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['GCS_PIPELINE_FILE_LOCATION'] = GCS_PIPELINE_FILE_LOCATION\n",
    "os.environ['PARAMETER_NAMES'] = PARAMETER_NAMES\n",
    "\n",
    "parameters = {\n",
    "    'file_uri': 'path/to/trigger/trigger/dummy.txt',\n",
    "    'unused_param': 0}\n",
    "\n",
    "message = base64.b64encode(json.dumps(parameters).encode())\n",
    "main.trigger_pipeline(\n",
    "    event={'data': message},\n",
    "    context=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c922432",
   "metadata": {},
   "source": [
    "## 1. Create a Pub/Sub topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ca3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud pubsub topics create {PUBSUB_TOPIC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6e34b",
   "metadata": {},
   "source": [
    "## 2. Deploy the Cloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dash separated pipeline parameter names\n",
    "PARAMETER_NAMES='num_epochs-hidden_units-learning_rate-batch_size'\n",
    "\n",
    "ENV_VARS=f\"\"\"\\\n",
    "PROJECT={PROJECT_ID},\\\n",
    "REGION={REGION},\\\n",
    "GCS_PIPELINE_FILE_LOCATION={GCS_PIPELINE_FILE_LOCATION},\\\n",
    "PARAMETER_NAMES={PARAMETER_NAMES}\n",
    "\"\"\"\n",
    "\n",
    "! echo {ENV_VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r src/pipeline_triggering/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdf2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions deploy {CLOUD_FUNCTION_NAME} \\\n",
    "    --region={REGION} \\\n",
    "    --trigger-topic={PUBSUB_TOPIC} \\\n",
    "    --runtime=python37 \\\n",
    "    --source=src/pipeline_triggering\\\n",
    "    --entry-point=trigger_pipeline\\\n",
    "    --stage-bucket={BUCKET_NAME}\\\n",
    "    --update-env-vars={ENV_VARS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac46acc",
   "metadata": {},
   "source": [
    "## 3. Test Triggering the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458edbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub\n",
    "\n",
    "publish_client = pubsub.PublisherClient()\n",
    "topic = f'projects/{PROJECT_ID}/topics/{PUBSUB_TOPIC}'\n",
    "data = {\n",
    "    'source_uri': 'pubsub/function/pipline',\n",
    "    'num_epochs': 4,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 512,\n",
    "    'hidden_units': '256,126'\n",
    "}\n",
    "message = json.dumps(data)\n",
    "\n",
    "_ = publish_client.publish(topic, message.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc7c6e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
