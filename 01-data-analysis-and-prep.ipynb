{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fossil-cream",
   "metadata": {},
   "source": [
    "# 01 - Data Analysis and Preparation\n",
    "\n",
    "This notebook covers the following tasks:\n",
    "\n",
    "1. Perform Exploratory Data Analysis and Visualization.\n",
    "2. Prepare the data for the ML task in BigQuery.\n",
    "3. Create a managed dataset.\n",
    "4. Produce and fix the raw data schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-consequence",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The [Chicago Taxi Trips](https://pantheon.corp.google.com/marketplace/details/city-of-chicago-public-data/chicago-taxi-trips) dataset is one ofof [public datasets hosted with BigQuery](https://cloud.google.com/bigquery/public-data/), which includes taxi trips from 2013 to the present, reported to the City of Chicago in its role as a regulatory agency. The `taxi_trips` table size is 70.72 GB and includes more than 195 million records. The dataset includes information about the trips, like pickup and dropoff datetime and location, passengers count, miles travelled, and trip toll. \n",
    "\n",
    "The ML task is to predict whether a given trip will result in a tip > 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-algeria",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.cloud import bigquery\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud.aiplatform import gapic as aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'\n",
    "REGION = 'us-central1'\n",
    "BQ_DATASET_NAME = 'playground_us'\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_prep'\n",
    "\n",
    "API_ENDPOINT = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "PARENT = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "DATASET_DISPLAYNAME = 'chicago_taxi_tips'\n",
    "BQ_URI = f\"bq://{PROJECT}.{BQ_DATASET_NAME}.{BQ_TABLE_NAME}\"\n",
    "\n",
    "client_options = {\"api_endpoint\": API_ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-local",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_WORKSPACE = '_workspace'\n",
    "LOCAL_DATA_DIR = os.path.join(LOCAL_WORKSPACE, 'csv_data')\n",
    "RAW_SCHEMA_DIR = 'model_src/raw_schema'\n",
    "REMOVE_WORKSPACE = True\n",
    "\n",
    "if tf.io.gfile.exists(LOCAL_WORKSPACE) and REMOVE_WORKSPACE:\n",
    "    print(\"Removing previous local workspace...\")\n",
    "    tf.io.gfile.rmtree(LOCAL_WORKSPACE)\n",
    "\n",
    "print(\"Creating new local workspace...\")\n",
    "tf.io.gfile.mkdir(LOCAL_WORKSPACE)\n",
    "print(\"Creating data directory...\")\n",
    "tf.io.gfile.mkdir(LOCAL_DATA_DIR)\n",
    "    \n",
    "tf.io.gfile.mkdir(RAW_SCHEMA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=US mk -d \\\n",
    "$PROJECT:$BQ_DATASET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-davis",
   "metadata": {},
   "source": [
    "## 1. Explore the Data in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-inventory",
   "metadata": {},
   "source": [
    "## 2. Create data for the ML task\n",
    "\n",
    "We add `data_split` column, where 80% of the records is set to `UNASSIGNED` while the other 20% is set to `TEST`.\n",
    "This column will be used by the AutoML Tables and the custom model to split the data for learning and testing.\n",
    "In the learning phase, each model will split the `UNASSIGNED` records to `train` and `eval`. The `TEST` split is the same for\n",
    "bot models for fair comparison in the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000000\n",
    "year = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = '''\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TABLE` \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      CONCAT(\n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "              pickup_latitude), 0.1)), \n",
    "          ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "              dropoff_latitude), 0.1))\n",
    "      ) AS loc_cross,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      IF(RAND() > 0.8, 'UNASSIGNED', 'TEST') AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = sql_script.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TABLE', BQ_TABLE_NAME).replace(\n",
    "    '@YEAR', str(year)).replace(\n",
    "    '@LIMIT', str(sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sql_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client()\n",
    "job = bq_client.query(sql_script)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT data_split, COUNT(*)\n",
    "FROM ksalama-cloudml.playground_us.chicago_taxitrips_prep\n",
    "GROUP BY data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-stability",
   "metadata": {},
   "source": [
    "### Save data locally as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-listing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery sample_data\n",
    "\n",
    "SELECT * EXCEPT (data_split)\n",
    "FROM ksalama-cloudml.playground_us.chicago_taxitrips_prep\n",
    "#LIMIT 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-first",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.tip_bin.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.euclidean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = os.path.join(LOCAL_DATA_DIR,'sample_data.csv')\n",
    "sample_data.to_csv(DATA_FILE, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-conservative",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l $DATA_FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-andrews",
   "metadata": {},
   "source": [
    "## 3. Create Managed AI Platform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-wheat",
   "metadata": {},
   "source": [
    "### Create managed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_client = aip.DatasetServiceClient(\n",
    "    client_options=client_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {\n",
    "    \"input_config\": {\n",
    "        \"bigquery_source\": {\"uri\": BQ_URI}\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata = json_format.ParseDict(metadata_dict, Value())\n",
    "\n",
    "dataset_desc = {\n",
    "    \"display_name\": DATASET_DISPLAYNAME,\n",
    "    \"metadata_schema_uri\": \"gs://google-cloud-aiplatform/schema/dataset/metadata/tabular_1.0.0.yaml\",\n",
    "    \"metadata\": metadata,\n",
    "}\n",
    "    \n",
    "response = dataset_client.create_dataset(\n",
    "    parent=PARENT, dataset=dataset_desc)\n",
    "response.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-roman",
   "metadata": {},
   "source": [
    "### List datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dataset_client.list_datasets(parent=PARENT)\n",
    "dataset_uri = None\n",
    "for dataset in datasets:\n",
    "    if dataset.display_name == DATASET_DISPLAYNAME:\n",
    "        dataset_uri = dataset.name\n",
    "        break\n",
    "        \n",
    "print(\"Dataset uri:\", dataset_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_client.get_dataset(name=dataset_uri)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.metadata['inputConfig']['bigquerySource']['uri']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-mention",
   "metadata": {},
   "source": [
    "## 4. Generate Raw Data Schema\n",
    "\n",
    "The raw data schema will be used in:\n",
    "1. Defining the input columns for the AutoML Tables model.\n",
    "2. Indentifying the raw data types and shapes in the data transformation.\n",
    "3. Create the serving input signature for the custom model.\n",
    "4. Validating the new raw training data in the tfx pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = tfdv.generate_statistics_from_csv(\n",
    "    data_location=DATA_FILE, \n",
    "    column_names=list(sample_data.columns), # CSV data file include header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-editor",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_schema_location = os.path.join(RAW_SCHEMA_DIR, 'schema.pbtxt')\n",
    "tfdv.write_schema_text(schema, raw_schema_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-detector",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
