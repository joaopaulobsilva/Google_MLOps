{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assisted-rental",
   "metadata": {},
   "source": [
    "# 07 - Continuous Training\n",
    "\n",
    "After testing, compiling, and uploading the pipeline definition to Cloud Storage, the pipeline is executed with respect to a trigger. We use [Cloud Functions](https://cloud.google.com/functions) and [Cloud Pub/Sub](https://cloud.google.com/pubsub) as a triggering mechanism. The triggering can be scheduled using [Cloud Schedular](https://cloud.google.com/scheduler). The trigger source sends a message to a Cloud Pub/Sub topic that the Cloud Function listens to, and then it submits the pipeline to AI Platform Managed Pipelines to be executed.\n",
    "\n",
    "This notebook covers the following steps:\n",
    "1. Create the Cloud Pub/Sub topic.\n",
    "2. Deploy the Cloud Function \n",
    "3. Test triggering a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-guess",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import tfx\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "print(\"Tensorflow Version:\", tfx.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml' # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us' # Change to your bucket.\n",
    "\n",
    "PIPELINE_NAME = 'chicago_taxi-tips-train-pipeline'\n",
    "GCS_PIPELINE_FILE_LOCATION = f'gs://{BUCKET}/ucaip_demo/chicago_taxi/complied_pipelines/{PIPELINE_NAME}.json'\n",
    "PUBSUB_TOPIC = f'trigger-{PIPELINE_NAME}'\n",
    "CLOUD_FUNCTION_NAME = f'trigger-{PIPELINE_NAME}-fn'\n",
    "PARAMETER_NAMES='num_epochs,hidden_units,learning_rate'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-cisco",
   "metadata": {},
   "source": [
    "## (Optional) Create a Dummy Pipeline for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMMY_PIPELINE_ROOT =  f\"gs://{BUCKET}/ucaip_demo/dummy_pipeline/local_runner\"\n",
    "PIPELINE_NAME = 'my_dummy_pipeline'\n",
    "PARAMETER_NAMES = 'source_uri'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-environment",
   "metadata": {},
   "source": [
    "### Implement the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfx\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.components.common_nodes.importer_node import ImporterNode\n",
    "from tfx.types.experimental.simple_artifacts import File\n",
    "\n",
    "def create_dummy_pipeline(\n",
    "    metadata_connection_config: metadata_store_pb2.ConnectionConfig, \n",
    "    pipeline_root: str,\n",
    "    source_uri: tfx.orchestration.data_types.RuntimeParameter = 'default_dummy_path',\n",
    "):\n",
    "    importer = ImporterNode(\n",
    "        source_uri=source_uri,\n",
    "        artifact_type=File\n",
    "    )\n",
    "    \n",
    "    return tfx.orchestration.pipeline.Pipeline(\n",
    "        pipeline_name='my-dummy-pipeline',\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[importer],\n",
    "        metadata_connection_config=metadata_connection_config\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-lodge",
   "metadata": {},
   "source": [
    "### Run the pipeline locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MLMD_SQLLITE = 'mlmd.sqllite'\n",
    "\n",
    "print(f\"artifacts location: {DUMMY_PIPELINE_ROOT}\")\n",
    "\n",
    "if tf.io.gfile.exists(DUMMY_PIPELINE_ROOT):\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.io.gfile.rmtree(DUMMY_PIPELINE_ROOT)\n",
    "\n",
    "if tf.io.gfile.exists(MLMD_SQLLITE):\n",
    "    print(\"Removing local mlmd SQLite...\")\n",
    "    tf.io.gfile.remove(MLMD_SQLLITE)\n",
    "\n",
    "metadata_connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "metadata_connection_config.sqlite.filename_uri = MLMD_SQLLITE\n",
    "metadata_connection_config.sqlite.connection_mode = 3\n",
    "print(\"ML metadata store is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.local.local_dag_runner import LocalDagRunner\n",
    "runner = LocalDagRunner()\n",
    "\n",
    "dummy_pipeline = create_dummy_pipeline(\n",
    "    metadata_connection_config=metadata_connection_config,\n",
    "    pipeline_root=DUMMY_PIPELINE_ROOT,\n",
    "    source_uri='path/to/dummpy/file.txt'\n",
    ")\n",
    "\n",
    "runner.run(dummy_pipeline)\n",
    "\n",
    "print(\"Pipeline finished exection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-accuracy",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.kubeflow.v2 import kubeflow_v2_dag_runner\n",
    "\n",
    "dummy_pipeline_definition_file = f'{PIPELINE_NAME}.json'\n",
    "\n",
    "dummy_pipeline = create_dummy_pipeline(\n",
    "    metadata_connection_config=metadata_connection_config,\n",
    "    pipeline_root=DUMMY_PIPELINE_ROOT,\n",
    "    source_uri=tfx.orchestration.data_types.RuntimeParameter(\n",
    "        name='source_uri',\n",
    "        default='path/to/default/dummy.txt',\n",
    "        ptype=str,\n",
    "    )\n",
    ")\n",
    "\n",
    "runner = kubeflow_v2_dag_runner.KubeflowV2DagRunner(\n",
    "    config=kubeflow_v2_dag_runner.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=dummy_pipeline_definition_file\n",
    ")\n",
    "    \n",
    "runner.run(dummy_pipeline, write_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-accounting",
   "metadata": {},
   "source": [
    "### Upload pipeline to Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCS_PIPELINE_FILE_LOCATION = f'gs://{BUCKET}/ucaip_demo/dummy_pipeline/complied_pipelines/{PIPELINE_NAME}.json'\n",
    "!gsutil cp {PIPELINE_NAME}.json {GCS_PIPELINE_FILE_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-clock",
   "metadata": {},
   "source": [
    "### Trigger the pipeline on AI Platform Managed Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from src.pipeline_triggering import main\n",
    "import base64\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['GCS_PIPELINE_FILE_LOCATION'] = GCS_PIPELINE_FILE_LOCATION\n",
    "os.environ['PARAMETER_NAMES'] = PARAMETER_NAMES\n",
    "\n",
    "parameters = {\n",
    "    'source_uri': 'source_uri_parameter_value',\n",
    "    'unused_param': 0}\n",
    "\n",
    "message = base64.b64encode(json.dumps(parameters).encode())\n",
    "main.trigger_pipeline(\n",
    "    event={'data': message},\n",
    "    context=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-stack",
   "metadata": {},
   "source": [
    "## 1. Create a Pub/Sub topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud pubsub topics create {PUBSUB_TOPIC}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-preparation",
   "metadata": {},
   "source": [
    "## 2. Deploy the Cloud Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_VARS=f\"\"\"\\\n",
    "PROJECT={PROJECT},\\\n",
    "REGION={REGION},\\\n",
    "GCS_PIPELINE_FILE_LOCATION={GCS_PIPELINE_FILE_LOCATION},\\\n",
    "PARAMETER_NAMES={PARAMETER_NAMES}\n",
    "\"\"\"\n",
    "\n",
    "!echo {ENV_VARS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r src/pipeline_triggering/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud functions deploy {CLOUD_FUNCTION_NAME} \\\n",
    "    --region={REGION} \\\n",
    "    --trigger-topic={PUBSUB_TOPIC} \\\n",
    "    --runtime=python37 \\\n",
    "    --source=src/pipeline_triggering\\\n",
    "    --entry-point=trigger_pipeline\\\n",
    "    --stage-bucket={BUCKET}\\\n",
    "    --update-env-vars={ENV_VARS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-marina",
   "metadata": {},
   "source": [
    "## 3. Test Triggering the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-marks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import pubsub\n",
    "import json\n",
    "\n",
    "publish_client = pubsub.PublisherClient()\n",
    "topic = f'projects/{PROJECT}/topics/{PUBSUB_TOPIC}'\n",
    "data = {\n",
    "    'source_uri': 'pubsub/function/pipline'\n",
    "}\n",
    "message = json.dumps(data)\n",
    "\n",
    "_ = publish_client.publish(topic, message.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-external",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
