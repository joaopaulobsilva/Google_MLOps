{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9dea698",
   "metadata": {},
   "source": [
    "# 03 - TFX Interactive Training Pipeline Execution\n",
    "\n",
    "The purpose of this notebook is to interactively run the following TFX pipeline steps:\n",
    "1. Receive hyperparameters using hyperparam_gen custom Python component.\n",
    "2. Extract data from BigQuery using BigQueryExampleGen components.\n",
    "3. Validate the raw data using StatisticsGen and ExampleValidator components.\n",
    "4. Process the data using Transform component.\n",
    "5. Train a custom model using Trainer component.\n",
    "7. Evaluate and Validate the custom model using ModelEvaluator\n",
    "7. Save the blessed model to the model registry location using using Pusher component.\n",
    "8. Upload the model to Vertex AI using aip_model_pusher custom Python component.\n",
    "\n",
    "The custom components are implemented in the [tfx_pipeline/components.py](tfx_pipeline/components) module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74adb899",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2223769f",
   "metadata": {},
   "source": [
    "Install the latest GA version of *google-cloud-storage* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1fbc5",
   "metadata": {},
   "source": [
    "Install deep learning dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d85ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U tfx==0.30.0 $USER_FLAG\n",
    "! pip3 install -r requirements.txt $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eeed3d",
   "metadata": {},
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the Vertex SDK and Google *cloud-storage*, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb75bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05e161",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tfx\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import logging\n",
    "\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer import executor as trainer_executor\n",
    "from tfx.dsl.components.common.resolver import Resolver\n",
    "from tfx.dsl.experimental import latest_artifacts_resolver\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "\n",
    "from src.common import features\n",
    "from src.model_training import data\n",
    "from src.tfx_pipelines import components\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "print(\"Tensorflow TFX Version:\", tfx.__version__)\n",
    "print(\"Tensorflow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea979bd",
   "metadata": {},
   "source": [
    "### Setup your Google Cloud project\n",
    "\n",
    "Enter your project ID in the cell below. Then run the  cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8834f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)\n",
    "    \n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ea0e9",
   "metadata": {},
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex. Not all regions provide support for all Vertex services. For the latest support per region, see the [Vertex locations documentation](https://cloud.google.com/ai-platform-unified/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4180b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  #@param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f3557",
   "metadata": {},
   "source": [
    "### Setup a bucket for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b08fabf",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0a717",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598e5a7",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1296ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd07f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DISPLAY_NAME = 'chicago_taxi_tips'\n",
    "MODEL_DISPLAY_NAME = f'{DATASET_DISPLAY_NAME}_classifier'\n",
    "\n",
    "WORKSPACE = f\"{BUCKET_NAME}/vertex_demo/\"\n",
    "RAW_SCHEMA_DIR = 'src/raw_schema'\n",
    "\n",
    "MLMD_SQLLITE = 'mlmd.sqllite'\n",
    "ARTIFACT_STORE = os.path.join(WORKSPACE, 'tfx_artifacts_interactive')\n",
    "MODEL_REGISTRY = os.path.join(WORKSPACE, 'model_registry')\n",
    "PIPELINE_NAME = f'{MODEL_DISPLAY_NAME}_training_pipeline'\n",
    "PIPELINE_ROOT = os.path.join(ARTIFACT_STORE, PIPELINE_NAME)\n",
    "print(\"Pipeline artifacts location:\", PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc76671",
   "metadata": {},
   "source": [
    "## Create Interactive Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225fa222",
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_ARTIFACTS = True\n",
    "if tf.io.gfile.exists(ARTIFACT_STORE) and REMOVE_ARTIFACTS:\n",
    "    print(\"Removing previous artifacts...\")\n",
    "    tf.io.gfile.rmtree(ARTIFACT_STORE)\n",
    "    \n",
    "if tf.io.gfile.exists(MLMD_SQLLITE) and REMOVE_ARTIFACTS:\n",
    "    print(\"Deleting previous mlmd.sqllite...\")\n",
    "    tf.io.gfile.rmtree(MLMD_SQLLITE)\n",
    "    \n",
    "print(f'Pipeline artifacts directory: {PIPELINE_ROOT}')\n",
    "print(f'Local metadata SQLlit path: {MLMD_SQLLITE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f896112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_metadata as mlmd\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "connection_config.sqlite.filename_uri = MLMD_SQLLITE\n",
    "connection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\n",
    "mlmd_store = mlmd.metadata_store.MetadataStore(connection_config)\n",
    "\n",
    "context = InteractiveContext(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  metadata_connection_config=connection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf15d085",
   "metadata": {},
   "source": [
    "## 1. Hyperparameter Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f527e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams_gen = components.hyperparameters_gen(\n",
    "    num_epochs=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    hidden_units='64,64',\n",
    ")\n",
    "\n",
    "context.run(hyperparams_gen, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab4799",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(\n",
    "    tf.io.gfile.GFile(\n",
    "        os.path.join(\n",
    "            hyperparams_gen.outputs.hyperparameters.get()[0].uri, 'hyperparameters.json')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10034cc",
   "metadata": {},
   "source": [
    "## 2. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894c75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import datasource_utils\n",
    "from tfx.extensions.google_cloud_big_query.example_gen.component import BigQueryExampleGen\n",
    "from tfx.proto import example_gen_pb2, transform_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efd899",
   "metadata": {},
   "source": [
    "### Extract train and eval splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35df4333",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = datasource_utils.get_training_source_query(\n",
    "    PROJECT_ID, REGION, DATASET_DISPLAY_NAME, ML_use='UNASSIGNED', limit=10000)\n",
    "\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(\n",
    "        splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=4),\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=1),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "train_example_gen = BigQueryExampleGen(query=sql_query, output_config=output_config)\n",
    "\n",
    "beam_pipeline_args=[\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--temp_location={BUCKET_NAME}/bq_tmp\"\n",
    "]\n",
    "\n",
    "context.run(\n",
    "    train_example_gen,\n",
    "    beam_pipeline_args=beam_pipeline_args,\n",
    "    enable_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5bafc9",
   "metadata": {},
   "source": [
    "### Extract test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = datasource_utils.get_training_source_query(\n",
    "    PROJECT_ID, REGION, DATASET_DISPLAY_NAME, ML_use='TEST', limit=1000)\n",
    "\n",
    "output_config = example_gen_pb2.Output(\n",
    "    split_config=example_gen_pb2.SplitConfig(\n",
    "        splits=[\n",
    "            example_gen_pb2.SplitConfig.Split(name=\"test\", hash_buckets=1),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "test_example_gen = BigQueryExampleGen(query=sql_query, output_config=output_config)\n",
    "\n",
    "beam_pipeline_args=[\n",
    "    f\"--project={PROJECT_ID}\",\n",
    "    f\"--temp_location={BUCKET_NAME}/bq_tmp\"\n",
    "]\n",
    "\n",
    "context.run(\n",
    "    test_example_gen,\n",
    "    beam_pipeline_args=beam_pipeline_args,\n",
    "    enable_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20f633",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = os.path.join(train_example_gen.outputs.examples.get()[0].uri, \"Split-train/*\")\n",
    "print(train_uri)\n",
    "\n",
    "source_raw_schema = tfdv.load_schema_text(os.path.join(RAW_SCHEMA_DIR, 'schema.pbtxt'))\n",
    "raw_feature_spec = schema_utils.schema_as_feature_spec(source_raw_schema).feature_spec\n",
    "\n",
    "def _parse_tf_example(tfrecord):\n",
    "    return tf.io.parse_single_example(tfrecord, raw_feature_spec)\n",
    "\n",
    "tfrecord_filenames = tf.data.Dataset.list_files(train_uri)\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "dataset = dataset.map(_parse_tf_example)\n",
    "\n",
    "for raw_features in dataset.shuffle(1000).batch(3).take(1):\n",
    "    for key in raw_features:\n",
    "        print(f\"{key}: {np.squeeze(raw_features[key], -1)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca44166",
   "metadata": {},
   "source": [
    "## 3. Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136a636",
   "metadata": {},
   "source": [
    "### Import raw schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e829b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_importer = tfx.components.common_nodes.importer_node.ImporterNode(\n",
    "    source_uri=RAW_SCHEMA_DIR,\n",
    "    artifact_type=tfx.types.standard_artifacts.Schema,\n",
    "    reimport=False\n",
    ")\n",
    "\n",
    "context.run(schema_importer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e7421d",
   "metadata": {},
   "source": [
    "### Generate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358de4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples=train_example_gen.outputs.examples)\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3909921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r {RAW_SCHEMA_DIR}/.ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530112f",
   "metadata": {},
   "source": [
    "### Validate statistics against schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26babae",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs.statistics,\n",
    "    schema=schema_importer.outputs.result,\n",
    ")\n",
    "\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584bc354",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs.anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376eb9e3",
   "metadata": {},
   "source": [
    "## 4. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa306767",
   "metadata": {},
   "outputs": [],
   "source": [
    "_transform_module_file = 'src/preprocessing/transformations.py'\n",
    "\n",
    "transform = tfx.components.Transform(\n",
    "    examples=train_example_gen.outputs.examples,\n",
    "    schema=schema_importer.outputs.result,\n",
    "    module_file=_transform_module_file,\n",
    "    splits_config=transform_pb2.SplitsConfig(\n",
    "        analyze=['train'], transform=['train', 'eval']),\n",
    ")\n",
    "\n",
    "context.run(transform, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31328329",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = os.path.join(transform.outputs.transformed_examples.get()[0].uri, \"Split-train/*\")\n",
    "transform_graph_uri = transform.outputs.transform_graph.get()[0].uri\n",
    "\n",
    "tft_output = tft.TFTransformOutput(transform_graph_uri)\n",
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_uri, transform_feature_spec, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} ({input_features[key].dtype}): {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ef115",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ea5f4",
   "metadata": {},
   "source": [
    "### Get the latest model to warm start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a582834",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model_resolver = Resolver(\n",
    "    strategy_class=latest_artifacts_resolver.LatestArtifactsResolver,\n",
    "    latest_model=tfx.types.Channel(type=tfx.types.standard_artifacts.Model)\n",
    ")\n",
    "\n",
    "context.run(latest_model_resolver, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16ee6ca",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f8e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_module_file = 'src/model_training/runner.py'\n",
    "\n",
    "trainer = tfx.components.Trainer(\n",
    "    custom_executor_spec=executor_spec.ExecutorClassSpec(trainer_executor.GenericExecutor),\n",
    "    module_file=_train_module_file,\n",
    "    transformed_examples=transform.outputs.transformed_examples,\n",
    "    schema=schema_importer.outputs.result,\n",
    "    base_model=latest_model_resolver.outputs.latest_model,\n",
    "    transform_graph=transform.outputs.transform_graph,\n",
    "    train_args=tfx.proto.trainer_pb2.TrainArgs(num_steps=0),\n",
    "    eval_args=tfx.proto.trainer_pb2.EvalArgs(num_steps=None),\n",
    "    hyperparameters=hyperparams_gen.outputs.hyperparameters,\n",
    ")\n",
    "\n",
    "context.run(trainer, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cdc56b",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09015c01",
   "metadata": {},
   "source": [
    "### Get the latest blessed model for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fadf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blessed_model_resolver = Resolver(\n",
    "    strategy_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
    "    model=tfx.types.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "    model_blessing=tfx.types.Channel(type=tfx.types.standard_artifacts.ModelBlessing)\n",
    ")\n",
    "\n",
    "context.run(blessed_model_resolver, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61bb0ea",
   "metadata": {},
   "source": [
    "### Evaluate and validate the model against the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[\n",
    "        tfma.ModelSpec(\n",
    "            signature_name='serving_tf_example',\n",
    "            label_key=features.TARGET_FEATURE_NAME,\n",
    "            prediction_key='probabilities')\n",
    "    ],\n",
    "    slicing_specs=[\n",
    "        tfma.SlicingSpec(),\n",
    "    ],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(\n",
    "            metrics=[   \n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(\n",
    "                    class_name='BinaryAccuracy',\n",
    "                    threshold=tfma.MetricThreshold(\n",
    "                        value_threshold=tfma.GenericValueThreshold(\n",
    "                            lower_bound={'value': 0.8}),\n",
    "                        # Change threshold will be ignored if there is no\n",
    "                        # baseline model resolved from MLMD (first run).\n",
    "                        change_threshold=tfma.GenericChangeThreshold(\n",
    "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                            absolute={'value': -1e-10}))),\n",
    "        ])\n",
    "    ])\n",
    "\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    examples=test_example_gen.outputs.examples,\n",
    "    example_splits=['test'],\n",
    "    model=trainer.outputs.model,\n",
    "    baseline_model=blessed_model_resolver.outputs.model,\n",
    "    eval_config=eval_config,\n",
    "    schema=schema_importer.outputs.result\n",
    ")\n",
    "\n",
    "context.run(evaluator, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluator.outputs.evaluation.get()[0].uri\n",
    "print(\"validation_ok:\", tfma.load_validation_result(evaluation_results).validation_ok)\n",
    "print(\"\")\n",
    "\n",
    "for entry in list(tfma.load_metrics(evaluation_results))[0].metric_keys_and_values:\n",
    "    value = entry.value.double_value.value\n",
    "    if value:\n",
    "        print(entry.key.name, \":\", round(entry.value.double_value.value, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536eccbd",
   "metadata": {},
   "source": [
    "## 7. Model Pushing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model_location = os.path.join(MODEL_REGISTRY, MODEL_DISPLAY_NAME)\n",
    "\n",
    "push_destination=tfx.proto.pusher_pb2.PushDestination(\n",
    "    filesystem=tfx.proto.pusher_pb2.PushDestination.Filesystem(\n",
    "        base_directory=exported_model_location,\n",
    "    )\n",
    ")\n",
    "\n",
    "pusher = tfx.components.Pusher(\n",
    "    model=trainer.outputs.model,\n",
    "    model_blessing=evaluator.outputs.blessing,\n",
    "    push_destination=push_destination\n",
    ")\n",
    "\n",
    "context.run(pusher, enable_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d84a0d",
   "metadata": {},
   "source": [
    "## 8. Model Upload to AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be5e2cf",
   "metadata": {},
   "source": [
    "### Generate the Explaination metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75be469",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_config = features.generate_explanation_config()\n",
    "print(explanation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af843313",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_runtime ='tf2-cpu.2-4'\n",
    "serving_image_uri = f\"gcr.io/cloud-aiplatform/prediction/{serving_runtime}:latest\"\n",
    "\n",
    "vertex_model_uploader = components.vertex_model_uploader(\n",
    "    project=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    pushed_model_location=exported_model_location,\n",
    "    serving_image_uri=serving_image_uri,\n",
    "    explanation_config=str(explanation_config)\n",
    ")\n",
    "\n",
    "context.run(vertex_model_uploader, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdc930",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_model_uploader.outputs.uploaded_model.get()[0].get_string_custom_property('model_uri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c677d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
