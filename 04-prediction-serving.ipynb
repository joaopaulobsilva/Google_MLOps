{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "proper-seeking",
   "metadata": {},
   "source": [
    "# 04 - Prediction Serving\n",
    "\n",
    "The purpose of the notebook is to show how to use the deployed model for online and batch prediction.\n",
    "The notebook covers the following tasks:\n",
    "1. Test the endpoints for online prediction.\n",
    "2. Use the uploaded custom model for batch prediciton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-money",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'  # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us' # Change to your bucket.\n",
    "\n",
    "BQ_DATASET_NAME = 'playground_us' # Change to your serving BigQuery dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_prep' # Change to your serving BigQuery table name.\n",
    "MODEL_DISPLAY_NAME = 'chicago_taxi_tips_classifier_v1'\n",
    "ENDPOINT_DISPLAY_NAME = 'chicago_taxi_tips_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.vertex_utils import VertexClient\n",
    "vertex_client = VertexClient(PROJECT, REGION, BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-tension",
   "metadata": {},
   "source": [
    "## 1. Making Online Predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = [  \n",
    "    \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"loc_cross\": [\"\"],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vertex_client.predict(\n",
    "    ENDPOINT_DISPLAY_NAME,\n",
    "    test_instances).predictions\n",
    "\n",
    "for prediction in predictions:\n",
    "    print(prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-rental",
   "metadata": {},
   "source": [
    "## 2. Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = f\"gs://{BUCKET}/ucaip_demo/chicago_taxi\"\n",
    "SERVING_DATA_DIR = os.path.join(WORKSPACE, 'serving_data')\n",
    "SERVING_INPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'input_data')\n",
    "SERVING_OUTPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'output_predictions')\n",
    "\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(SERVING_DATA_DIR):\n",
    "    print(\"Removing previous serving data...\")\n",
    "    tf.io.gfile.rmtree(SERVING_DATA_DIR)\n",
    "print(\"Creating preprocessing serving data directory...\")\n",
    "tf.io.gfile.mkdir(SERVING_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-stomach",
   "metadata": {},
   "source": [
    "### Extract serving data to Cloud Storage as TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-accident",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import datasource_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'TEST'\n",
    "LIMIT = 10000\n",
    "\n",
    "raw_data_query = datasource_utils.get_serving_source_query(\n",
    "    bq_dataset_name=BQ_DATASET_NAME, \n",
    "    bq_table_name=BQ_TABLE_NAME,\n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-traveler",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    #'runner': 'DataflowRunner',\n",
    "    'raw_schema_location': RAW_SCHEMA_LOCATION,\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': os.path.join(SERVING_INPUT_DATA_DIR, \"data-\"),\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-liberal",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Data extraction started...\")\n",
    "etl.run_extract_pipeline(args)\n",
    "print(\"Data extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-breach",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_INPUT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-congo",
   "metadata": {},
   "source": [
    "### Prepare the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_resources =  {\n",
    "    \"machine_type\": 'n1-standard-2',\n",
    "    #'accelerator_count': 1,\n",
    "    #'accelerator_type': 'NVIDIA_TESLA_T4'\n",
    "    \"starting_replica_count\": 1,\n",
    "    \"max_replica_count\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-algorithm",
   "metadata": {},
   "source": [
    "### Submit the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = vertex_client.submit_batch_prediction_job(\n",
    "    model_display_name=MODEL_DISPLAY_NAME, \n",
    "    gcs_source_pattern=SERVING_INPUT_DATA_DIR + '/*.jsonl', \n",
    "    gcs_destination_prefix=SERVING_OUTPUT_DATA_DIR,\n",
    "    instances_format='jsonl',\n",
    "    predictions_format='jsonl',\n",
    "    other_configurations=job_resources,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_OUTPUT_DATA_DIR}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-batch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
