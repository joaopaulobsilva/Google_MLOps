{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-newark",
   "metadata": {},
   "source": [
    "# 04 - Prediction Serving\n",
    "\n",
    "The purpose of the notebook is to show how to use the deployed model for online and batch prediction.\n",
    "The notebook covers the following tasks:\n",
    "1. Test the endpoints for online prediction.\n",
    "2. Use the uploaded custom model for batch prediciton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-wiring",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'ksalama-cloudml'  # Change to your project Id.\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'ksalama-cloudml-us' # Change to your bucket.\n",
    "\n",
    "BQ_DATASET_NAME = 'playground_us' # Change to your serving BigQuery dataset name.\n",
    "BQ_TABLE_NAME = 'chicago_taxitrips_prep' # Change to your serving BigQuery table name.\n",
    "MODEL_DISPLAY_NAME = 'chicago_taxi_tips_classifier_v1'\n",
    "ENDPOINT_DISPLAY_NAME = 'chicago_taxi_tips_classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.vertex_utils import VertexUtils\n",
    "vertex_utils = VertexUtils(PROJECT, REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-salon",
   "metadata": {},
   "source": [
    "## 1. Making Online Predicitons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = vertex_utils.get_endpoint_by_display_name(ENDPOINT_DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = {\n",
    "    \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "    \"euclidean\": [2064.2696],\n",
    "    \"loc_cross\": [\"\"],\n",
    "    \"payment_type\": [\"Credit Card\"],\n",
    "    \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "    \"trip_miles\": [1.37],\n",
    "    \"trip_day\": [12],\n",
    "    \"trip_hour\": [16],\n",
    "    \"trip_month\": [2],\n",
    "    \"trip_day_of_week\": [4],\n",
    "    \"trip_seconds\": [555],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    response = vertex_utils.predict_tabular_classifier(\n",
    "        endpoint.name,\n",
    "        test_instance)\n",
    "    \n",
    "    print(f\"Model {response.deployed_model_id}) responded:\")\n",
    "    for prediction in response.predictions:\n",
    "        print(dict(prediction))\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-gender",
   "metadata": {},
   "source": [
    "## 2. Batch Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-biotechnology",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = f\"gs://{BUCKET}/ucaip_demo/chicago_taxi\"\n",
    "SERVING_DATA_DIR = os.path.join(WORKSPACE, 'serving_data')\n",
    "SERVING_INPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'input_data')\n",
    "SERVING_OUTPUT_DATA_DIR = os.path.join(SERVING_DATA_DIR, 'output_predictions')\n",
    "\n",
    "RAW_SCHEMA_LOCATION = 'src/raw_schema/schema.pbtxt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.io.gfile.exists(SERVING_DATA_DIR):\n",
    "    print(\"Removing previous serving data...\")\n",
    "    tf.io.gfile.rmtree(SERVING_DATA_DIR)\n",
    "print(\"Creating preprocessing serving data directory...\")\n",
    "tf.io.gfile.mkdir(SERVING_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-bhutan",
   "metadata": {},
   "source": [
    "### Extract serving data to Cloud Storage as TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import datasource_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLIT = 'TEST'\n",
    "LIMIT = 10000\n",
    "\n",
    "raw_data_query = datasource_utils.get_serving_source_query(\n",
    "    bq_dataset_name=BQ_DATASET_NAME, \n",
    "    bq_table_name=BQ_TABLE_NAME,\n",
    "    limit=LIMIT\n",
    ")\n",
    "\n",
    "print(raw_data_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-gallery",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    #'runner': 'DataflowRunner',\n",
    "    'raw_schema_location': RAW_SCHEMA_LOCATION,\n",
    "    'raw_data_query': raw_data_query,\n",
    "    'exported_data_prefix': os.path.join(SERVING_INPUT_DATA_DIR, \"data-\"),\n",
    "    'temporary_dir': os.path.join(WORKSPACE, 'tmp'),\n",
    "    'gcs_location': os.path.join(WORKSPACE, 'bq_tmp'),\n",
    "    'project': PROJECT,\n",
    "    'region': REGION,\n",
    "    'setup_file': './setup.py'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "print(\"Data extraction started...\")\n",
    "etl.run_extract_pipeline(args)\n",
    "print(\"Data extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_INPUT_DATA_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-there",
   "metadata": {},
   "source": [
    "### Prepare the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedicated_resources =  {\n",
    "    \"machine_spec\": {\n",
    "        \"machine_type\": 'n1-standard-2',\n",
    "        #'accelerator_count': 1,\n",
    "        #'accelerator_type': 'NVIDIA_TESLA_T4'\n",
    "    },\n",
    "    \"starting_replica_count\": 1,\n",
    "    \"max_replica_count\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-smoke",
   "metadata": {},
   "source": [
    "### Submit the batch prediction job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = vertex_utils.submit_batch_prediction_job(\n",
    "    model_display_name=MODEL_DISPLAY_NAME, \n",
    "    gcs_data_uri_pattern=SERVING_INPUT_DATA_DIR + '/*.jsonl', \n",
    "    gcs_output_uri=SERVING_OUTPUT_DATA_DIR,\n",
    "    dedicated_resources=dedicated_resources,\n",
    "    instances_format='jsonl',\n",
    "    predictions_format='jsonl'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-deposit",
   "metadata": {},
   "source": [
    "### Monitor job state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    response = vertex_utils.get_batch_prediction_job_by_uri(batch_prediction_job.name)\n",
    "    if response.state.name == 'JOB_STATE_SUCCEEDED':\n",
    "        print(\"Batch prediction completed. - Training Time:\", response.update_time - response.create_time)\n",
    "        break\n",
    "    elif response.state.name == 'JOB_STATE_FAILED':\n",
    "        print(\"Batch prediction failed!\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Batch prediction state is: {response.state.name}.\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls {SERVING_OUTPUT_DATA_DIR}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
